#!/usr/bin/env python3
"""
Enhanced Variance Analysis Visualization Script for ComparisonIK

This script reads the enhanced variance analysis CSV data generated by the ComparisonIK application
and creates comprehensive visualizations of execution time distributions, variance metrics,
clearance analysis, orientation noise robustness, and performance analysis.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import argparse
import sys

# Set style for better-looking plots
plt.style.use('seaborn-v0_8')
sns.set_palette("husl")

def load_variance_data(csv_file):
    """Load variance analysis data from CSV file"""
    try:
        df = pd.read_csv(csv_file)
        print(f"Loaded {len(df)} records from {csv_file}")
        print(f"Columns: {list(df.columns)}")
        print(f"Unique poses: {df['pose_index'].nunique()}")
        print(f"Trials per pose: {len(df[df['pose_index'] == df['pose_index'].iloc[0]])}")
        return df
    except FileNotFoundError:
        print(f"Error: File {csv_file} not found")
        return None
    except Exception as e:
        print(f"Error loading data: {e}")
        return None

def plot_execution_time_distribution(df, output_dir):
    """Create execution time distribution plots"""
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))
    fig.suptitle('Execution Time Distribution Analysis', fontsize=16, fontweight='bold')
    
    # 1. Histogram of all execution times
    axes[0, 0].hist(df['solve_time'], bins=30, alpha=0.7, edgecolor='black')
    axes[0, 0].set_xlabel('Execution Time (seconds)')
    axes[0, 0].set_ylabel('Frequency')
    axes[0, 0].set_title('Distribution of All Execution Times')
    axes[0, 0].grid(True, alpha=0.3)
    
    # Add statistics text
    mean_time = df['solve_time'].mean()
    std_time = df['solve_time'].std()
    axes[0, 0].axvline(mean_time, color='red', linestyle='--', label=f'Mean: {mean_time:.4f}s')
    axes[0, 0].axvline(mean_time + std_time, color='orange', linestyle='--', alpha=0.7, label=f'±1σ')
    axes[0, 0].axvline(mean_time - std_time, color='orange', linestyle='--', alpha=0.7)
    axes[0, 0].legend()
    
    # 2. Box plot by pose
    pose_data = [df[df['pose_index'] == pose]['solve_time'].values 
                 for pose in sorted(df['pose_index'].unique())]
    box_plot = axes[0, 1].boxplot(pose_data, patch_artist=True)
    
    # Color the boxes
    colors = plt.cm.Set3(np.linspace(0, 1, len(box_plot['boxes'])))
    for patch, color in zip(box_plot['boxes'], colors):
        patch.set_facecolor(color)
    
    axes[0, 1].set_xlabel('Pose Index')
    axes[0, 1].set_ylabel('Execution Time (seconds)')
    axes[0, 1].set_title('Execution Time Distribution by Pose')
    axes[0, 1].set_xticklabels([f'Pose {i}' for i in sorted(df['pose_index'].unique())])
    axes[0, 1].grid(True, alpha=0.3)
    
    # 3. Time series plot for each pose
    for pose in sorted(df['pose_index'].unique()):
        pose_data = df[df['pose_index'] == pose].sort_values('trial_number')
        axes[1, 0].plot(pose_data['trial_number'], pose_data['solve_time'], 
                       marker='o', alpha=0.7, label=f'Pose {pose}')
    
    axes[1, 0].set_xlabel('Trial Number')
    axes[1, 0].set_ylabel('Execution Time (seconds)')
    axes[1, 0].set_title('Execution Time Trends Across Trials')
    axes[1, 0].legend()
    axes[1, 0].grid(True, alpha=0.3)
    
    # 4. Violin plot showing distribution shape
    pose_indices = sorted(df['pose_index'].unique())
    violin_data = [df[df['pose_index'] == pose]['solve_time'].values for pose in pose_indices]
    
    violin_parts = axes[1, 1].violinplot(violin_data, positions=range(1, len(pose_indices) + 1))
    
    axes[1, 1].set_xlabel('Pose Index')
    axes[1, 1].set_ylabel('Execution Time (seconds)')
    axes[1, 1].set_title('Execution Time Distribution Shape by Pose')
    axes[1, 1].set_xticks(range(1, len(pose_indices) + 1))
    axes[1, 1].set_xticklabels([f'Pose {i}' for i in pose_indices])
    axes[1, 1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    output_file = output_dir / 'execution_time_distribution.png'
    plt.savefig(output_file, dpi=300, bbox_inches='tight')
    print(f"Saved: {output_file}")
    plt.show()

def plot_variance_metrics(df, output_dir):
    """Create variance metrics visualization"""
    # Get summary statistics for each pose
    agg_dict = {
        'mean_solve_time': 'first',
        'std_dev_solve_time': 'first',
        'median_solve_time': 'first',
        'min_solve_time': 'first',
        'max_solve_time': 'first',
        'coefficient_of_variation': 'first',
        'success_rate': 'first'
    }
    
    # Add enhanced metrics if available
    if 'max_clearance' in df.columns:
        agg_dict.update({
            'max_clearance': 'first',
            'std_dev_clearance': 'first',
            'clearance_range': 'first',
            'has_clearance_violation': 'first'
        })
    
    if 'noise_robustness_score' in df.columns:
        agg_dict['noise_robustness_score'] = 'first'
    
    pose_stats = df.groupby('pose_index').agg(agg_dict).reset_index()
    
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    fig.suptitle('Variance Analysis Metrics by Pose', fontsize=16, fontweight='bold')
    
    # 1. Mean vs Standard Deviation
    axes[0, 0].scatter(pose_stats['mean_solve_time'], pose_stats['std_dev_solve_time'], 
                      s=100, alpha=0.7, c=pose_stats['pose_index'], cmap='viridis')
    axes[0, 0].set_xlabel('Mean Execution Time (s)')
    axes[0, 0].set_ylabel('Standard Deviation (s)')
    axes[0, 0].set_title('Mean vs Standard Deviation')
    axes[0, 0].grid(True, alpha=0.3)
    
    # Add pose labels
    for _, row in pose_stats.iterrows():
        axes[0, 0].annotate(f'P{int(row["pose_index"])}', 
                           (row['mean_solve_time'], row['std_dev_solve_time']),
                           xytext=(5, 5), textcoords='offset points', fontsize=8)
    
    # 2. Coefficient of Variation
    bars = axes[0, 1].bar(range(len(pose_stats)), pose_stats['coefficient_of_variation'], 
                         alpha=0.7, color=plt.cm.viridis(np.linspace(0, 1, len(pose_stats))))
    axes[0, 1].set_xlabel('Pose Index')
    axes[0, 1].set_ylabel('Coefficient of Variation')
    axes[0, 1].set_title('Coefficient of Variation by Pose')
    axes[0, 1].set_xticks(range(len(pose_stats)))
    axes[0, 1].set_xticklabels([f'P{int(p)}' for p in pose_stats['pose_index']])
    axes[0, 1].grid(True, alpha=0.3, axis='y')
    
    # Add value labels on bars
    for bar, cv in zip(bars, pose_stats['coefficient_of_variation']):
        axes[0, 1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001,
                       f'{cv:.3f}', ha='center', va='bottom', fontsize=8)
    
    # 3. Range (Max - Min)
    ranges = pose_stats['max_solve_time'] - pose_stats['min_solve_time']
    bars = axes[0, 2].bar(range(len(pose_stats)), ranges, alpha=0.7,
                         color=plt.cm.plasma(np.linspace(0, 1, len(pose_stats))))
    axes[0, 2].set_xlabel('Pose Index')
    axes[0, 2].set_ylabel('Range (Max - Min) (s)')
    axes[0, 2].set_title('Execution Time Range by Pose')
    axes[0, 2].set_xticks(range(len(pose_stats)))
    axes[0, 2].set_xticklabels([f'P{int(p)}' for p in pose_stats['pose_index']])
    axes[0, 2].grid(True, alpha=0.3, axis='y')
    
    # 4. Success Rate
    bars = axes[1, 0].bar(range(len(pose_stats)), pose_stats['success_rate'], 
                         alpha=0.7, color=plt.cm.RdYlGn(pose_stats['success_rate']/100))
    axes[1, 0].set_xlabel('Pose Index')
    axes[1, 0].set_ylabel('Success Rate (%)')
    axes[1, 0].set_title('Success Rate by Pose')
    axes[1, 0].set_xticks(range(len(pose_stats)))
    axes[1, 0].set_xticklabels([f'P{int(p)}' for p in pose_stats['pose_index']])
    axes[1, 0].set_ylim(0, 105)
    axes[1, 0].grid(True, alpha=0.3, axis='y')
    
    # Add percentage labels on bars
    for bar, rate in zip(bars, pose_stats['success_rate']):
        axes[1, 0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,
                       f'{rate*100:.1f}%', ha='center', va='bottom', fontsize=8)
    
    # 5. Mean vs Median comparison
    x_pos = np.arange(len(pose_stats))
    width = 0.35
    
    bars1 = axes[1, 1].bar(x_pos - width/2, pose_stats['mean_solve_time'], width, 
                          label='Mean', alpha=0.7, color='skyblue')
    bars2 = axes[1, 1].bar(x_pos + width/2, pose_stats['median_solve_time'], width,
                          label='Median', alpha=0.7, color='lightcoral')
    
    axes[1, 1].set_xlabel('Pose Index')
    axes[1, 1].set_ylabel('Execution Time (s)')
    axes[1, 1].set_title('Mean vs Median Execution Time')
    axes[1, 1].set_xticks(x_pos)
    axes[1, 1].set_xticklabels([f'P{int(p)}' for p in pose_stats['pose_index']])
    axes[1, 1].legend()
    axes[1, 1].grid(True, alpha=0.3, axis='y')
    
    # 6. Enhanced correlation heatmap of metrics
    correlation_cols = ['mean_solve_time', 'std_dev_solve_time', 'coefficient_of_variation', 'success_rate']
    correlation_labels = ['Mean Time', 'Std Dev', 'CV', 'Success Rate']
    
    # Add enhanced metrics if available
    if 'max_clearance' in pose_stats.columns and (pose_stats['max_clearance'] >= 0).any():
        correlation_cols.extend(['max_clearance', 'std_dev_clearance', 'clearance_range'])
        correlation_labels.extend(['Max Clearance', 'Clearance Std', 'Clearance Range'])
    
    if 'noise_robustness_score' in pose_stats.columns and (pose_stats['noise_robustness_score'] > 0).any():
        correlation_cols.append('noise_robustness_score')
        correlation_labels.append('Noise Robustness')
    
    # Filter out invalid data for correlation
    valid_pose_stats = pose_stats.copy()
    for col in correlation_cols:
        if col in valid_pose_stats.columns:
            valid_pose_stats = valid_pose_stats[valid_pose_stats[col] >= 0]
    
    correlation_data = valid_pose_stats[correlation_cols].corr()
    
    im = axes[1, 2].imshow(correlation_data, cmap='coolwarm', aspect='auto', vmin=-1, vmax=1)
    axes[1, 2].set_xticks(range(len(correlation_data.columns)))
    axes[1, 2].set_yticks(range(len(correlation_data.columns)))
    axes[1, 2].set_xticklabels(correlation_labels, rotation=45, ha='right')
    axes[1, 2].set_yticklabels(correlation_labels)
    axes[1, 2].set_title('Enhanced Metric Correlation Matrix')
    
    # Add correlation values to the heatmap
    for i in range(len(correlation_data.columns)):
        for j in range(len(correlation_data.columns)):
            text = axes[1, 2].text(j, i, f'{correlation_data.iloc[i, j]:.2f}',
                                  ha="center", va="center", 
                                  color="black" if abs(correlation_data.iloc[i, j]) < 0.5 else "white")
    
    # Add colorbar
    cbar = plt.colorbar(im, ax=axes[1, 2], shrink=0.8)
    cbar.set_label('Correlation Coefficient')
    
    plt.tight_layout()
    output_file = output_dir / 'variance_metrics.png'
    plt.savefig(output_file, dpi=300, bbox_inches='tight')
    print(f"Saved: {output_file}")
    plt.show()

def plot_performance_summary(df, output_dir):
    """Create performance summary visualization"""
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))
    fig.suptitle('Performance Summary Analysis', fontsize=16, fontweight='bold')
    
    # Calculate statistics for each pose
    pose_stats = df.groupby('pose_index').agg({
        'solve_time': ['mean', 'std', 'min', 'max', 'count'],
        'collision_check_time': 'mean',
        'clearance_compute_time': 'mean',
        'success': 'mean',
        'collision_free': 'mean'
    }).round(6)
    
    # Flatten column names
    pose_stats.columns = ['_'.join(col).strip() for col in pose_stats.columns.values]
    pose_stats = pose_stats.reset_index()
    
    # 1. Execution time components stacked bar chart
    components = ['solve_time_mean', 'collision_check_time_mean', 'clearance_compute_time_mean']
    component_labels = ['Solve Time', 'Collision Check', 'Clearance Compute']
    
    bottom = np.zeros(len(pose_stats))
    colors = ['skyblue', 'lightcoral', 'lightgreen']
    
    for i, (component, label, color) in enumerate(zip(components, component_labels, colors)):
        if component in pose_stats.columns:
            axes[0, 0].bar(range(len(pose_stats)), pose_stats[component], 
                          bottom=bottom, label=label, alpha=0.8, color=color)
            bottom += pose_stats[component]
    
    axes[0, 0].set_xlabel('Pose Index')
    axes[0, 0].set_ylabel('Time (seconds)')
    axes[0, 0].set_title('Execution Time Components')
    axes[0, 0].set_xticks(range(len(pose_stats)))
    axes[0, 0].set_xticklabels([f'P{int(p)}' for p in pose_stats['pose_index']])
    axes[0, 0].legend()
    axes[0, 0].grid(True, alpha=0.3, axis='y')
    
    # 2. Execution time with error bars
    axes[0, 1].errorbar(range(len(pose_stats)), pose_stats['solve_time_mean'], 
                       yerr=pose_stats['solve_time_std'], fmt='o-', capsize=5, 
                       capthick=2, alpha=0.8, linewidth=2, markersize=8)
    axes[0, 1].set_xlabel('Pose Index')
    axes[0, 1].set_ylabel('Execution Time (seconds)')
    axes[0, 1].set_title('Mean Execution Time with Standard Deviation')
    axes[0, 1].set_xticks(range(len(pose_stats)))
    axes[0, 1].set_xticklabels([f'P{int(p)}' for p in pose_stats['pose_index']])
    axes[0, 1].grid(True, alpha=0.3)
    
    # 3. Min/Max range visualization
    axes[1, 0].fill_between(range(len(pose_stats)), 
                           pose_stats['solve_time_min'], 
                           pose_stats['solve_time_max'], 
                           alpha=0.3, label='Min-Max Range')
    axes[1, 0].plot(range(len(pose_stats)), pose_stats['solve_time_mean'], 
                   'o-', linewidth=2, markersize=8, label='Mean')
    
    axes[1, 0].set_xlabel('Pose Index')
    axes[1, 0].set_ylabel('Execution Time (seconds)')
    axes[1, 0].set_title('Execution Time Range (Min-Max)')
    axes[1, 0].set_xticks(range(len(pose_stats)))
    axes[1, 0].set_xticklabels([f'P{int(p)}' for p in pose_stats['pose_index']])
    axes[1, 0].legend()
    axes[1, 0].grid(True, alpha=0.3)
    
    # 4. Performance consistency (CV) vs Success Rate
    cv_data = df.groupby('pose_index')['coefficient_of_variation'].first()
    success_data = df.groupby('pose_index')['success_rate'].first()
    
    scatter = axes[1, 1].scatter(cv_data, success_data * 100, 
                                s=150, alpha=0.7, c=pose_stats['pose_index'], 
                                cmap='viridis', edgecolors='black', linewidth=1)
    
    axes[1, 1].set_xlabel('Coefficient of Variation')
    axes[1, 1].set_ylabel('Success Rate (%)')
    axes[1, 1].set_title('Performance Consistency vs Success Rate')
    axes[1, 1].grid(True, alpha=0.3)
    
    # Add pose labels
    for pose, cv, success in zip(pose_stats['pose_index'], cv_data, success_data):
        axes[1, 1].annotate(f'P{int(pose)}', (cv, success * 100),
                           xytext=(5, 5), textcoords='offset points', fontsize=10)
    
    plt.tight_layout()
    output_file = output_dir / 'performance_summary.png'
    plt.savefig(output_file, dpi=300, bbox_inches='tight')
    print(f"Saved: {output_file}")
    plt.show()

def generate_statistics_report(df, output_dir):
    """Generate a comprehensive statistics report"""
    report_file = output_dir / 'enhanced_variance_analysis_report.txt'
    
    # Detect enhanced format
    has_enhanced, has_noise_testing = detect_enhanced_format(df)
    
    with open(report_file, 'w') as f:
        f.write("ENHANCED VARIANCE ANALYSIS REPORT\n")
        f.write("=" * 50 + "\n\n")
        
        f.write(f"Report Format: {'Enhanced' if has_enhanced else 'Standard'}\n")
        f.write(f"Noise Testing: {'Enabled' if has_noise_testing else 'Disabled'}\n\n")
        
        # Overall statistics
        f.write("OVERALL STATISTICS:\n")
        f.write(f"Total number of trials: {len(df)}\n")
        f.write(f"Number of poses tested: {df['pose_index'].nunique()}\n")
        f.write(f"Trials per pose: {len(df[df['pose_index'] == df['pose_index'].iloc[0]])}\n")
        f.write(f"Overall success rate: {df['success'].mean() * 100:.2f}%\n")
        f.write(f"Overall collision-free rate: {df['collision_free'].mean() * 100:.2f}%\n\n")
        
        # Execution time statistics
        f.write("EXECUTION TIME STATISTICS:\n")
        f.write(f"Mean execution time: {df['solve_time'].mean():.6f} seconds\n")
        f.write(f"Median execution time: {df['solve_time'].median():.6f} seconds\n")
        f.write(f"Standard deviation: {df['solve_time'].std():.6f} seconds\n")
        f.write(f"Minimum execution time: {df['solve_time'].min():.6f} seconds\n")
        f.write(f"Maximum execution time: {df['solve_time'].max():.6f} seconds\n")
        f.write(f"Overall coefficient of variation: {df['solve_time'].std() / df['solve_time'].mean():.6f}\n\n")
        
        # Enhanced clearance statistics
        if has_enhanced:
            f.write("ENHANCED CLEARANCE STATISTICS:\n")
            valid_clearance_data = df[df['max_clearance'] >= 0]
            if len(valid_clearance_data) > 0:
                f.write(f"Mean minimum clearance: {valid_clearance_data['min_clearance'].mean():.6f} meters\n")
                f.write(f"Mean average clearance: {valid_clearance_data['avg_clearance'].mean():.6f} meters\n")
                f.write(f"Mean maximum clearance: {valid_clearance_data['max_clearance'].mean():.6f} meters\n")
                f.write(f"Mean clearance std deviation: {valid_clearance_data['std_dev_clearance'].mean():.6f} meters\n")
                f.write(f"Mean clearance range: {valid_clearance_data['clearance_range'].mean():.6f} meters\n")
                
                violation_rate = valid_clearance_data['has_clearance_violation'].mean() * 100
                f.write(f"Clearance violation rate: {violation_rate:.1f}%\n\n")
            else:
                f.write("No valid enhanced clearance data available\n\n")
        
        # Orientation noise statistics
        if has_noise_testing:
            f.write("ORIENTATION NOISE ROBUSTNESS STATISTICS:\n")
            noise_data = df[df['orientation_noise_magnitude'] > 0]
            if len(noise_data) > 0:
                f.write(f"Noise magnitude tested: {noise_data['orientation_noise_magnitude'].iloc[0]:.3f} radians\n")
                f.write(f"Mean robustness score: {noise_data['noise_robustness_score'].mean():.3f}\n")
                f.write(f"Success rate with noise: {noise_data['success_rate'].mean():.2f}%\n")
                
                # Compare with normal conditions
                normal_data = df[df['orientation_noise_magnitude'] == 0]
                if len(normal_data) > 0:
                    normal_success = normal_data['success_rate'].mean()
                    noise_success = noise_data['success_rate'].mean()
                    success_degradation = normal_success - noise_success
                    f.write(f"Success rate degradation due to noise: {success_degradation:.2f} percentage points\n\n")
            else:
                f.write("No orientation noise testing data available\n\n")
        
        # Per-pose statistics
        f.write("PER-POSE DETAILED STATISTICS:\n")
        
        # Build aggregation dictionary based on available columns
        agg_dict = {
            'solve_time': ['mean', 'std', 'min', 'max', 'median'],
            'success': 'mean',
            'collision_free': 'mean',
            'coefficient_of_variation': 'first'
        }
        
        if has_enhanced:
            agg_dict.update({
                'max_clearance': 'first',
                'std_dev_clearance': 'first', 
                'clearance_range': 'first',
                'has_clearance_violation': 'first'
            })
        
        if has_noise_testing:
            agg_dict['noise_robustness_score'] = 'first'
        
        pose_stats = df.groupby('pose_index').agg(agg_dict).round(6)
        
        for pose in sorted(df['pose_index'].unique()):
            pose_data = df[df['pose_index'] == pose]
            f.write(f"\nPose {pose}:\n")
            f.write(f"  Mean execution time: {pose_data['solve_time'].mean():.6f} seconds\n")
            f.write(f"  Standard deviation: {pose_data['solve_time'].std():.6f} seconds\n")
            f.write(f"  Coefficient of variation: {pose_data['coefficient_of_variation'].iloc[0]:.6f}\n")
            f.write(f"  Success rate: {pose_data['success'].mean() * 100:.2f}%\n")
            f.write(f"  Collision-free rate: {pose_data['collision_free'].mean() * 100:.2f}%\n")
            f.write(f"  Min/Max time: {pose_data['solve_time'].min():.6f} / {pose_data['solve_time'].max():.6f} seconds\n")
            
            if has_enhanced:
                clearance_data = pose_data[pose_data['max_clearance'] >= 0]
                if len(clearance_data) > 0:
                    f.write(f"  Enhanced clearance metrics:\n")
                    f.write(f"    Min/Avg/Max clearance: {clearance_data['min_clearance'].iloc[0]:.6f} / {clearance_data['avg_clearance'].iloc[0]:.6f} / {clearance_data['max_clearance'].iloc[0]:.6f} meters\n")
                    f.write(f"    Clearance std dev: {clearance_data['std_dev_clearance'].iloc[0]:.6f} meters\n")
                    f.write(f"    Clearance range: {clearance_data['clearance_range'].iloc[0]:.6f} meters\n")
                    f.write(f"    Has violation: {'Yes' if clearance_data['has_clearance_violation'].iloc[0] else 'No'}\n")
            
            if has_noise_testing:
                noise_data = pose_data[pose_data['orientation_noise_magnitude'] > 0]
                if len(noise_data) > 0:
                    f.write(f"  Noise robustness score: {noise_data['noise_robustness_score'].iloc[0]:.3f}\n")
                    f.write(f"  Success rate with noise: {noise_data['success_rate'].mean():.2f}%\n")
    
    print(f"Enhanced statistics report saved to: {report_file}")

def detect_enhanced_format(df):
    """Detect if the CSV contains enhanced format with new fields"""
    enhanced_fields = ['max_clearance', 'std_dev_clearance', 'clearance_range', 
                      'has_clearance_violation', 'orientation_noise_magnitude', 'noise_robustness_score']
    has_enhanced = all(field in df.columns for field in enhanced_fields)
    has_noise_testing = 'orientation_noise_magnitude' in df.columns and df['orientation_noise_magnitude'].max() > 0
    return has_enhanced, has_noise_testing

def plot_enhanced_clearance_analysis(df, output_dir):
    """Create enhanced clearance analysis plots"""
    fig, axes = plt.subplots(2, 3, figsize=(20, 12))
    fig.suptitle('Enhanced Clearance Analysis', fontsize=16, fontweight='bold')
    
    # Get summary statistics for each pose
    pose_stats = df.groupby('pose_index').agg({
        'min_clearance': 'first',
        'avg_clearance': 'first', 
        'max_clearance': 'first',
        'std_dev_clearance': 'first',
        'clearance_range': 'first',
        'has_clearance_violation': 'first'
    }).reset_index()
    
    # Filter out invalid clearance data (negative values indicate no data)
    valid_data = pose_stats[pose_stats['max_clearance'] >= 0].copy()
    
    if len(valid_data) == 0:
        # Show message if no enhanced clearance data available
        fig.text(0.5, 0.5, 'Enhanced clearance data not available\nRun with updated C++ version for enhanced metrics', 
                ha='center', va='center', fontsize=14, transform=fig.transFigure)
        plt.tight_layout()
        output_file = output_dir / 'enhanced_clearance_analysis.png'
        plt.savefig(output_file, dpi=300, bbox_inches='tight')
        print(f"Saved: {output_file}")
        plt.show()
        return
    
    # 1. Clearance range visualization (Min, Avg, Max)
    x_pos = np.arange(len(valid_data))
    width = 0.25
    
    bars1 = axes[0, 0].bar(x_pos - width, valid_data['min_clearance'], width, 
                          label='Min', alpha=0.8, color='lightcoral')
    bars2 = axes[0, 0].bar(x_pos, valid_data['avg_clearance'], width,
                          label='Avg', alpha=0.8, color='skyblue')
    bars3 = axes[0, 0].bar(x_pos + width, valid_data['max_clearance'], width,
                          label='Max', alpha=0.8, color='lightgreen')
    
    axes[0, 0].set_xlabel('Pose Index')
    axes[0, 0].set_ylabel('Clearance (m)')
    axes[0, 0].set_title('Clearance Distribution by Pose')
    axes[0, 0].set_xticks(x_pos)
    axes[0, 0].set_xticklabels([f'P{int(p)}' for p in valid_data['pose_index']])
    axes[0, 0].legend()
    axes[0, 0].grid(True, alpha=0.3, axis='y')
    
    # 2. Clearance standard deviation
    bars = axes[0, 1].bar(x_pos, valid_data['std_dev_clearance'], alpha=0.7,
                         color=plt.cm.plasma(np.linspace(0, 1, len(valid_data))))
    axes[0, 1].set_xlabel('Pose Index')
    axes[0, 1].set_ylabel('Clearance Std Dev (m)')
    axes[0, 1].set_title('Clearance Variability by Pose')
    axes[0, 1].set_xticks(x_pos)
    axes[0, 1].set_xticklabels([f'P{int(p)}' for p in valid_data['pose_index']])
    axes[0, 1].grid(True, alpha=0.3, axis='y')
    
    # Add value labels on bars
    for bar, std_val in zip(bars, valid_data['std_dev_clearance']):
        axes[0, 1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001,
                       f'{std_val:.4f}', ha='center', va='bottom', fontsize=8, rotation=90)
    
    # 3. Clearance range (Max - Min)
    bars = axes[0, 2].bar(x_pos, valid_data['clearance_range'], alpha=0.7,
                         color=plt.cm.viridis(np.linspace(0, 1, len(valid_data))))
    axes[0, 2].set_xlabel('Pose Index')
    axes[0, 2].set_ylabel('Clearance Range (m)')
    axes[0, 2].set_title('Clearance Range by Pose')
    axes[0, 2].set_xticks(x_pos)
    axes[0, 2].set_xticklabels([f'P{int(p)}' for p in valid_data['pose_index']])
    axes[0, 2].grid(True, alpha=0.3, axis='y')
    
    # 4. Clearance violations
    violation_counts = valid_data['has_clearance_violation'].astype(int)
    colors = ['green' if v == 0 else 'red' for v in violation_counts]
    bars = axes[1, 0].bar(x_pos, violation_counts, alpha=0.7, color=colors)
    axes[1, 0].set_xlabel('Pose Index')
    axes[1, 0].set_ylabel('Has Violation (1=Yes, 0=No)')
    axes[1, 0].set_title('Clearance Violations by Pose')
    axes[1, 0].set_xticks(x_pos)
    axes[1, 0].set_xticklabels([f'P{int(p)}' for p in valid_data['pose_index']])
    axes[1, 0].set_ylim(-0.1, 1.1)
    axes[1, 0].grid(True, alpha=0.3, axis='y')
    
    # Add violation percentage as text
    violation_rate = violation_counts.mean() * 100
    axes[1, 0].text(0.02, 0.98, f'Violation Rate: {violation_rate:.1f}%', 
                   transform=axes[1, 0].transAxes, verticalalignment='top',
                   bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))
    
    # 5. Clearance vs Execution Time correlation
    pose_exec_times = df.groupby('pose_index')['solve_time'].mean()
    valid_exec_times = pose_exec_times[pose_exec_times.index.isin(valid_data['pose_index'])]
    
    scatter = axes[1, 1].scatter(valid_data['avg_clearance'], valid_exec_times.values, 
                                s=100, alpha=0.7, c=valid_data['pose_index'], 
                                cmap='viridis', edgecolors='black', linewidth=1)
    axes[1, 1].set_xlabel('Average Clearance (m)')
    axes[1, 1].set_ylabel('Mean Execution Time (s)')
    axes[1, 1].set_title('Clearance vs Execution Time')
    axes[1, 1].grid(True, alpha=0.3)
    
    # Add pose labels
    for pose, clearance, exec_time in zip(valid_data['pose_index'], valid_data['avg_clearance'], valid_exec_times.values):
        axes[1, 1].annotate(f'P{int(pose)}', (clearance, exec_time),
                           xytext=(5, 5), textcoords='offset points', fontsize=8)
    
    # 6. Clearance distribution histogram
    all_clearances = []
    clearance_types = []
    
    for _, row in valid_data.iterrows():
        all_clearances.extend([row['min_clearance'], row['avg_clearance'], row['max_clearance']])
        clearance_types.extend(['Min', 'Avg', 'Max'])
    
    clearance_df = pd.DataFrame({'clearance': all_clearances, 'type': clearance_types})
    
    for clearance_type in ['Min', 'Avg', 'Max']:
        data = clearance_df[clearance_df['type'] == clearance_type]['clearance']
        axes[1, 2].hist(data, bins=15, alpha=0.6, label=clearance_type, density=True)
    
    axes[1, 2].set_xlabel('Clearance (m)')
    axes[1, 2].set_ylabel('Density')
    axes[1, 2].set_title('Clearance Distribution Across All Poses')
    axes[1, 2].legend()
    axes[1, 2].grid(True, alpha=0.3)
    
    plt.tight_layout()
    output_file = output_dir / 'enhanced_clearance_analysis.png'
    plt.savefig(output_file, dpi=300, bbox_inches='tight')
    print(f"Saved: {output_file}")
    plt.show()

def plot_orientation_noise_analysis(df, output_dir):
    """Create orientation noise robustness analysis plots"""
    # Check if noise testing data is available
    if 'orientation_noise_magnitude' not in df.columns or df['orientation_noise_magnitude'].max() == 0:
        print("No orientation noise testing data found. Run with --test-noise flag to generate noise data.")
        return
    
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))
    fig.suptitle('Orientation Noise Robustness Analysis', fontsize=16, fontweight='bold')
    
    # Get noise testing data
    noise_data = df[df['orientation_noise_magnitude'] > 0].copy()
    if len(noise_data) == 0:
        fig.text(0.5, 0.5, 'No orientation noise testing data available\nRun with --test-noise flag', 
                ha='center', va='center', fontsize=14, transform=fig.transFigure)
        plt.tight_layout()
        output_file = output_dir / 'orientation_noise_analysis.png'
        plt.savefig(output_file, dpi=300, bbox_inches='tight')
        print(f"Saved: {output_file}")
        plt.show()
        return
    
    # Get summary statistics for each pose
    pose_stats = noise_data.groupby('pose_index').agg({
        'orientation_noise_magnitude': 'first',
        'noise_robustness_score': 'first',
        'success_rate': 'first'
    }).reset_index()
    
    # 1. Noise robustness score by pose
    bars = axes[0, 0].bar(range(len(pose_stats)), pose_stats['noise_robustness_score'], 
                         alpha=0.7, color=plt.cm.RdYlGn(pose_stats['noise_robustness_score']))
    axes[0, 0].set_xlabel('Pose Index')
    axes[0, 0].set_ylabel('Robustness Score')
    axes[0, 0].set_title('Orientation Noise Robustness by Pose')
    axes[0, 0].set_xticks(range(len(pose_stats)))
    axes[0, 0].set_xticklabels([f'P{int(p)}' for p in pose_stats['pose_index']])
    axes[0, 0].set_ylim(0, 1.05)
    axes[0, 0].grid(True, alpha=0.3, axis='y')
    
    # Add score labels on bars
    for bar, score in zip(bars, pose_stats['noise_robustness_score']):
        axes[0, 0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,
                       f'{score:.2f}', ha='center', va='bottom', fontsize=9)
    
    # 2. Success rate with noise magnitude info
    normal_data = df[df['orientation_noise_magnitude'] == 0].groupby('pose_index')['success_rate'].first()
    noisy_success = pose_stats.set_index('pose_index')['success_rate']
    
    x_pos = np.arange(len(pose_stats))
    width = 0.6
    
    # Check if we have normal (no noise) data
    if len(normal_data) > 0:
        # We have both normal and noisy data - show comparison
        width = 0.35
        bars1 = axes[0, 1].bar(x_pos - width/2, normal_data.values, width, 
                              label='Normal', alpha=0.8, color='skyblue')
        bars2 = axes[0, 1].bar(x_pos + width/2, noisy_success.values, width,
                              label='With Noise', alpha=0.8, color='lightcoral')
        axes[0, 1].legend()
        title = 'Success Rate: Normal vs Noisy Conditions'
    else:
        # Only noisy data available - show noise magnitude
        noise_magnitude = pose_stats['orientation_noise_magnitude'].iloc[0]
        bars2 = axes[0, 1].bar(x_pos, noisy_success.values, width,
                              alpha=0.8, color='lightcoral')
        title = f'Success Rate with Noise (σ={noise_magnitude:.2f} rad)'
        
        # Add success rate labels on bars
        for bar, rate in zip(bars2, noisy_success.values):
            axes[0, 1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                           f'{rate:.1f}%', ha='center', va='bottom', fontsize=9)
    
    axes[0, 1].set_xlabel('Pose Index')
    axes[0, 1].set_ylabel('Success Rate (%)')
    axes[0, 1].set_title(title)
    axes[0, 1].set_xticks(x_pos)
    axes[0, 1].set_xticklabels([f'P{int(p)}' for p in pose_stats['pose_index']])
    axes[0, 1].set_ylim(0, 105)
    axes[0, 1].grid(True, alpha=0.3, axis='y')
    
    # 3. Robustness vs Success Rate correlation
    scatter = axes[1, 0].scatter(pose_stats['noise_robustness_score'], pose_stats['success_rate'], 
                                s=150, alpha=0.7, c=pose_stats['pose_index'], 
                                cmap='viridis', edgecolors='black', linewidth=1)
    
    axes[1, 0].set_xlabel('Noise Robustness Score')
    axes[1, 0].set_ylabel('Success Rate with Noise')
    axes[1, 0].set_title('Robustness Score vs Success Rate')
    axes[1, 0].grid(True, alpha=0.3)
    
    # Add trend line
    z = np.polyfit(pose_stats['noise_robustness_score'], pose_stats['success_rate'], 1)
    p = np.poly1d(z)
    axes[1, 0].plot(pose_stats['noise_robustness_score'], p(pose_stats['noise_robustness_score']), 
                   "r--", alpha=0.8, linewidth=2)
    
    # Add pose labels
    for pose, robustness, success in zip(pose_stats['pose_index'], pose_stats['noise_robustness_score'], pose_stats['success_rate']):
        axes[1, 0].annotate(f'P{int(pose)}', (robustness, success),
                           xytext=(5, 5), textcoords='offset points', fontsize=10)
    
    # 4. Noise magnitude vs execution time
    if 'solve_time' in noise_data.columns:
        noise_exec_times = noise_data.groupby('pose_index')['solve_time'].mean()
        
        scatter = axes[1, 1].scatter(pose_stats['orientation_noise_magnitude'], noise_exec_times.values, 
                                    s=150, alpha=0.7, c=pose_stats['noise_robustness_score'], 
                                    cmap='RdYlGn', edgecolors='black', linewidth=1)
        
        axes[1, 1].set_xlabel('Orientation Noise Magnitude (rad)')
        axes[1, 1].set_ylabel('Mean Execution Time (s)')
        axes[1, 1].set_title('Noise Impact on Execution Time')
        axes[1, 1].grid(True, alpha=0.3)
        
        # Add colorbar
        cbar = plt.colorbar(scatter, ax=axes[1, 1])
        cbar.set_label('Robustness Score')
        
        # Add pose labels
        for pose, noise_mag, exec_time in zip(pose_stats['pose_index'], pose_stats['orientation_noise_magnitude'], noise_exec_times.values):
            axes[1, 1].annotate(f'P{int(pose)}', (noise_mag, exec_time),
                               xytext=(5, 5), textcoords='offset points', fontsize=10)
    
    plt.tight_layout()
    output_file = output_dir / 'orientation_noise_analysis.png'
    plt.savefig(output_file, dpi=300, bbox_inches='tight')
    print(f"Saved: {output_file}")
    plt.show()

def main():
    parser = argparse.ArgumentParser(description='Visualize Enhanced ComparisonIK variance analysis results')
    parser.add_argument('--csv', default='ik_variance_analysis_results.csv', 
                       help='Path to variance analysis CSV file')
    parser.add_argument('--output-dir', default='.', 
                       help='Output directory for plots and reports')
    parser.add_argument('--skip-enhanced', action='store_true',
                       help='Skip enhanced plots if enhanced data not available')
    
    args = parser.parse_args()
    
    # Create output directory
    output_dir = Path(args.output_dir)
    output_dir.mkdir(exist_ok=True)
    
    # Load data
    df = load_variance_data(args.csv)
    if df is None:
        sys.exit(1)
    
    # Detect enhanced format
    has_enhanced, has_noise_testing = detect_enhanced_format(df)
    
    print(f"\nGenerating enhanced variance analysis visualizations...")
    print(f"Output directory: {output_dir.absolute()}")
    print(f"Enhanced format detected: {has_enhanced}")
    print(f"Noise testing data available: {has_noise_testing}")
    
    # Generate standard plots
    plot_execution_time_distribution(df, output_dir)
    plot_variance_metrics(df, output_dir)
    plot_performance_summary(df, output_dir)
    
    # Generate enhanced plots if data is available
    if has_enhanced and not args.skip_enhanced:
        plot_enhanced_clearance_analysis(df, output_dir)
    elif not has_enhanced:
        print("Enhanced clearance data not available. Run with updated C++ version for enhanced metrics.")
    
    if has_noise_testing and not args.skip_enhanced:
        plot_orientation_noise_analysis(df, output_dir)
    elif not has_noise_testing:
        print("Orientation noise testing data not available. Run C++ with --test-noise flag.")
    
    # Generate enhanced statistics report
    generate_statistics_report(df, output_dir)
    
    print("\nAll visualizations and reports generated successfully!")
    print(f"Check the output directory: {output_dir.absolute()}")
    
    if has_enhanced:
        print("✅ Enhanced clearance analysis available")
    if has_noise_testing:
        print("✅ Orientation noise robustness analysis available")
    if not has_enhanced and not has_noise_testing:
        print("💡 Tip: Run the C++ application with enhanced features for more detailed analysis")

if __name__ == "__main__":
    main()
