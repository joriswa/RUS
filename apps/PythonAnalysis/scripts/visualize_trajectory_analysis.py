#!/usr/bin/env python3
"""
Comprehensive Trajectory Planning Analysis Visualization Script

This script provides comprehensive visualization and analysis capabilities for trajectory planning
evaluation data generated by the enhanced SinglePoseEvaluator system.

Features:
- Performance metrics analysis and visualization
- Safety metrics assessment (clearance, time-to-collision)
- Trajectory planning quality metrics
- Dynamic feasibility analysis
- Statistical analysis and reporting
- Multi-trial comparison and consistency analysis
- Export capabilities for research publication

Author: Enhanced PathPlanner Analysis System
Date: 2024
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import argparse
import warnings
from datetime import datetime
import json
from scipy import stats
from matplotlib.patches import Rectangle
from matplotlib.gridspec import GridSpec

# Configure matplotlib and seaborn
plt.style.use('default')
sns.set_palette("husl")
warnings.filterwarnings('ignore')

class TrajectoryPlanningAnalyzer:
    """
    Comprehensive analyzer for trajectory planning evaluation data.
    Provides visualization, statistical analysis, and reporting capabilities.
    """
    
    def __init__(self, data_file):
        """Initialize analyzer with data file."""
        self.data_file = Path(data_file)
        self.data = None
        self.stats_summary = {}
        self.analysis_timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # Define metric categories
        self.performance_metrics = [
            'planning_time_ms', 'trajectory_length', 'smoothness', 'num_waypoints',
            'joint_distance', 'cartesian_distance', 'max_velocity', 'max_acceleration',
            'energy_consumption', 'execution_time'
        ]
        
        self.safety_metrics = [
            'min_clearance', 'avg_clearance', 'clearance_variance', 'critical_violations',
            'min_self_clearance', 'avg_self_clearance', 'clearance_margin_ratio',
            'clearance_path_ratio', 'min_ttc', 'avg_ttc', 'critical_ttc_violations'
        ]
        
        self.quality_metrics = [
            'min_manipulability', 'avg_manipulability', 'singularity_violations',
            'redundancy_utilization', 'max_joint_torque', 'avg_joint_torque',
            'dynamic_feasibility', 'velocity_variance', 'acceleration_variance'
        ]
        
        # Define acceptable ranges for safety assessment
        self.safety_thresholds = {
            'min_clearance': {'critical': 0.05, 'safe': 0.15},
            'min_ttc': {'critical': 1.0, 'safe': 3.0},
            'min_manipulability': {'critical': 0.01, 'safe': 0.1},
            'max_joint_torque': {'critical': 50.0, 'safe': 30.0}
        }
        
    def load_data(self):
        """Load and validate trajectory evaluation data."""
        try:
            self.data = pd.read_csv(self.data_file)
            print(f"âœ… Loaded {len(self.data)} trials from {self.data_file}")
            
            # Validate required columns
            required_cols = ['trial', 'success', 'algorithm']
            missing_cols = [col for col in required_cols if col not in self.data.columns]
            if missing_cols:
                raise ValueError(f"Missing required columns: {missing_cols}")
                
            # Basic data info
            print(f"ðŸ“Š Data overview:")
            print(f"   - Total trials: {len(self.data)}")
            print(f"   - Successful trials: {self.data['success'].sum()}")
            print(f"   - Success rate: {self.data['success'].mean()*100:.1f}%")
            print(f"   - Algorithms: {', '.join(self.data['algorithm'].unique())}")
            
            return True
            
        except Exception as e:
            print(f"âŒ Error loading data: {e}")
            return False
    
    def generate_summary_statistics(self):
        """Generate comprehensive summary statistics."""
        if self.data is None:
            print("âŒ No data loaded")
            return
            
        # Filter successful trials for analysis
        success_data = self.data[self.data['success'] == 1]
        
        if len(success_data) == 0:
            print("âŒ No successful trials found")
            return
            
        # Calculate statistics for each metric category
        self.stats_summary = {
            'overview': {
                'total_trials': len(self.data),
                'successful_trials': len(success_data),
                'success_rate': success_data['success'].mean() * 100,
                'algorithms': list(self.data['algorithm'].unique())
            }
        }
        
        # Performance metrics statistics
        self.stats_summary['performance'] = {}
        for metric in self.performance_metrics:
            if metric in success_data.columns:
                self.stats_summary['performance'][metric] = {
                    'mean': float(success_data[metric].mean()),
                    'std': float(success_data[metric].std()),
                    'min': float(success_data[metric].min()),
                    'max': float(success_data[metric].max()),
                    'median': float(success_data[metric].median())
                }
        
        # Safety metrics statistics
        self.stats_summary['safety'] = {}
        for metric in self.safety_metrics:
            if metric in success_data.columns:
                self.stats_summary['safety'][metric] = {
                    'mean': float(success_data[metric].mean()),
                    'std': float(success_data[metric].std()),
                    'min': float(success_data[metric].min()),
                    'max': float(success_data[metric].max()),
                    'median': float(success_data[metric].median())
                }
        
        # Quality metrics statistics
        self.stats_summary['quality'] = {}
        for metric in self.quality_metrics:
            if metric in success_data.columns:
                # Handle boolean dynamic_feasibility
                if metric == 'dynamic_feasibility':
                    self.stats_summary['quality'][metric] = {
                        'feasible_ratio': float(success_data[metric].mean()),
                        'total_feasible': int(success_data[metric].sum()),
                        'total_trials': len(success_data)
                    }
                else:
                    self.stats_summary['quality'][metric] = {
                        'mean': float(success_data[metric].mean()),
                        'std': float(success_data[metric].std()),
                        'min': float(success_data[metric].min()),
                        'max': float(success_data[metric].max()),
                        'median': float(success_data[metric].median())
                    }
        
        print("âœ… Summary statistics generated")
    
    def create_performance_analysis(self):
        """Create comprehensive performance analysis visualizations."""
        if self.data is None:
            return
            
        success_data = self.data[self.data['success'] == 1]
        if len(success_data) == 0:
            return
            
        # Create figure with subplots
        fig = plt.figure(figsize=(20, 15))
        gs = GridSpec(4, 4, figure=fig, hspace=0.3, wspace=0.3)
        
        # 1. Planning Time Analysis
        ax1 = fig.add_subplot(gs[0, 0])
        planning_times = success_data['planning_time_ms'] / 1000  # Convert to seconds
        ax1.hist(planning_times, bins=20, alpha=0.7, color='skyblue', edgecolor='black')
        ax1.set_xlabel('Planning Time (s)')
        ax1.set_ylabel('Frequency')
        ax1.set_title('Planning Time Distribution')
        ax1.axvline(planning_times.mean(), color='red', linestyle='--', 
                   label=f'Mean: {planning_times.mean():.1f}s')
        ax1.legend()
        
        # 2. Trajectory Quality Metrics
        ax2 = fig.add_subplot(gs[0, 1])
        quality_metrics = ['trajectory_length', 'smoothness', 'energy_consumption']
        available_metrics = [m for m in quality_metrics if m in success_data.columns]
        if available_metrics:
            quality_data = success_data[available_metrics].values
            im = ax2.imshow(quality_data.T, cmap='viridis', aspect='auto')
            ax2.set_yticks(range(len(available_metrics)))
            ax2.set_yticklabels(available_metrics)
            ax2.set_xlabel('Trial')
            ax2.set_title('Trajectory Quality Heatmap')
            plt.colorbar(im, ax=ax2)
        
        # 3. Velocity and Acceleration Analysis
        ax3 = fig.add_subplot(gs[0, 2])
        if 'max_velocity' in success_data.columns and 'max_acceleration' in success_data.columns:
            ax3.scatter(success_data['max_velocity'], success_data['max_acceleration'], 
                       alpha=0.7, s=50)
            ax3.set_xlabel('Max Velocity (m/s)')
            ax3.set_ylabel('Max Acceleration (m/sÂ²)')
            ax3.set_title('Velocity vs Acceleration')
            
        # 4. Execution Time Analysis
        ax4 = fig.add_subplot(gs[0, 3])
        if 'execution_time' in success_data.columns:
            execution_times = success_data['execution_time']
            ax4.boxplot(execution_times)
            ax4.set_ylabel('Execution Time (s)')
            ax4.set_title('Execution Time Distribution')
            ax4.text(1.1, execution_times.median(), f'Median: {execution_times.median():.1f}s',
                    verticalalignment='center')
        
        # 5. Algorithm Comparison (if multiple algorithms)
        ax5 = fig.add_subplot(gs[1, :2])
        algorithms = success_data['algorithm'].unique()
        if len(algorithms) > 1:
            metrics_to_compare = ['planning_time_ms', 'trajectory_length', 'smoothness']
            available_compare = [m for m in metrics_to_compare if m in success_data.columns]
            
            x = np.arange(len(available_compare))
            width = 0.35
            
            for i, alg in enumerate(algorithms):
                alg_data = success_data[success_data['algorithm'] == alg]
                means = [alg_data[m].mean() for m in available_compare]
                stds = [alg_data[m].std() for m in available_compare]
                
                ax5.bar(x + i*width, means, width, label=alg, alpha=0.7, yerr=stds, capsize=5)
            
            ax5.set_xlabel('Metrics')
            ax5.set_ylabel('Values')
            ax5.set_title('Algorithm Performance Comparison')
            ax5.set_xticks(x + width/2)
            ax5.set_xticklabels(available_compare, rotation=45)
            ax5.legend()
        else:
            ax5.text(0.5, 0.5, 'Single Algorithm Used\n(No Comparison Available)', 
                    ha='center', va='center', transform=ax5.transAxes, fontsize=12)
            ax5.set_title('Algorithm Analysis')
        
        # 6. Motion Variance Analysis
        ax6 = fig.add_subplot(gs[1, 2:])
        variance_metrics = ['velocity_variance', 'acceleration_variance']
        available_variance = [m for m in variance_metrics if m in success_data.columns]
        
        if available_variance:
            for i, metric in enumerate(available_variance):
                values = success_data[metric]
                ax6.hist(values, bins=15, alpha=0.7, label=metric.replace('_', ' ').title())
            ax6.set_xlabel('Variance')
            ax6.set_ylabel('Frequency')
            ax6.legend()
            ax6.set_title('Motion Variance Analysis')
        
        # 7. Performance Trends
        ax7 = fig.add_subplot(gs[2, :])
        if len(success_data) > 1:
            # Plot key metrics over trials
            trial_order = success_data['trial'].values
            metrics_to_plot = ['planning_time_ms', 'trajectory_length', 'smoothness']
            available_plot = [m for m in metrics_to_plot if m in success_data.columns]
            
            for metric in available_plot:
                # Normalize to 0-1 scale for comparison
                values = success_data[metric].values
                normalized_values = (values - values.min()) / (values.max() - values.min())
                ax7.plot(trial_order, normalized_values, marker='o', label=metric, linewidth=2)
            
            ax7.set_xlabel('Trial Number')
            ax7.set_ylabel('Normalized Value (0-1)')
            ax7.set_title('Performance Trends Across Trials')
            ax7.legend()
            ax7.grid(True, alpha=0.3)
        
        # 8. Statistical Summary Table
        ax8 = fig.add_subplot(gs[3, :])
        ax8.axis('off')
        
        # Create summary statistics table
        summary_data = []
        key_metrics = ['planning_time_ms', 'trajectory_length', 'smoothness', 'execution_time']
        available_key = [m for m in key_metrics if m in success_data.columns]
        
        for metric in available_key:
            values = success_data[metric]
            summary_data.append([
                metric.replace('_', ' ').title(),
                f"{values.mean():.3f}",
                f"{values.std():.3f}",
                f"{values.min():.3f}",
                f"{values.max():.3f}",
                f"{values.median():.3f}"
            ])
        
        if summary_data:
            table = ax8.table(cellText=summary_data,
                            colLabels=['Metric', 'Mean', 'Std Dev', 'Min', 'Max', 'Median'],
                            cellLoc='center',
                            loc='center',
                            bbox=[0, 0, 1, 1])
            table.auto_set_font_size(False)
            table.set_fontsize(10)
            table.scale(1, 2)
            ax8.set_title('Performance Statistics Summary', pad=20)
        
        plt.suptitle('Trajectory Planning Performance Analysis', fontsize=16, fontweight='bold')
        return fig
    
    def create_safety_analysis(self):
        """Create comprehensive safety analysis visualizations."""
        if self.data is None:
            return
            
        success_data = self.data[self.data['success'] == 1]
        if len(success_data) == 0:
            return
            
        # Create figure with subplots
        fig = plt.figure(figsize=(20, 15))
        gs = GridSpec(4, 3, figure=fig, hspace=0.4, wspace=0.3)
        
        # 1. Clearance Analysis
        ax1 = fig.add_subplot(gs[0, 0])
        clearance_metrics = ['min_clearance', 'avg_clearance']
        available_clearance = [m for m in clearance_metrics if m in success_data.columns]
        
        if available_clearance:
            for metric in available_clearance:
                values = success_data[metric]
                ax1.hist(values, bins=15, alpha=0.6, label=metric.replace('_', ' ').title())
            
            # Add safety threshold lines
            if 'critical' in self.safety_thresholds.get('min_clearance', {}):
                ax1.axvline(self.safety_thresholds['min_clearance']['critical'], 
                           color='red', linestyle='--', label='Critical Threshold')
            if 'safe' in self.safety_thresholds.get('min_clearance', {}):
                ax1.axvline(self.safety_thresholds['min_clearance']['safe'], 
                           color='green', linestyle='--', label='Safe Threshold')
            
            ax1.set_xlabel('Clearance (m)')
            ax1.set_ylabel('Frequency')
            ax1.set_title('Clearance Distribution')
            ax1.legend()
        
        # 2. Time-to-Collision Analysis
        ax2 = fig.add_subplot(gs[0, 1])
        ttc_metrics = ['min_ttc', 'avg_ttc']
        available_ttc = [m for m in ttc_metrics if m in success_data.columns]
        
        if available_ttc:
            for metric in available_ttc:
                values = success_data[metric]
                ax2.hist(values, bins=15, alpha=0.6, label=metric.replace('_', ' ').title())
            
            # Add safety threshold lines
            if 'critical' in self.safety_thresholds.get('min_ttc', {}):
                ax2.axvline(self.safety_thresholds['min_ttc']['critical'], 
                           color='red', linestyle='--', label='Critical Threshold')
            if 'safe' in self.safety_thresholds.get('min_ttc', {}):
                ax2.axvline(self.safety_thresholds['min_ttc']['safe'], 
                           color='green', linestyle='--', label='Safe Threshold')
            
            ax2.set_xlabel('Time to Collision (s)')
            ax2.set_ylabel('Frequency')
            ax2.set_title('Time-to-Collision Distribution')
            ax2.legend()
        
        # 3. Safety Violations Overview
        ax3 = fig.add_subplot(gs[0, 2])
        violation_metrics = ['critical_violations', 'critical_ttc_violations', 'singularity_violations']
        available_violations = [m for m in violation_metrics if m in success_data.columns]
        
        if available_violations:
            violation_counts = []
            violation_labels = []
            for metric in available_violations:
                count = success_data[metric].sum()
                violation_counts.append(count)
                violation_labels.append(metric.replace('_', ' ').title())
            
            bars = ax3.bar(violation_labels, violation_counts, color=['red', 'orange', 'yellow'])
            ax3.set_ylabel('Number of Violations')
            ax3.set_title('Safety Violations Summary')
            ax3.tick_params(axis='x', rotation=45)
            
            # Add value labels on bars
            for bar, count in zip(bars, violation_counts):
                height = bar.get_height()
                ax3.text(bar.get_x() + bar.get_width()/2., height + 0.1,
                        f'{int(count)}', ha='center', va='bottom')
        
        # 4. Clearance vs Time-to-Collision Scatter
        ax4 = fig.add_subplot(gs[1, 0])
        if 'min_clearance' in success_data.columns and 'min_ttc' in success_data.columns:
            scatter = ax4.scatter(success_data['min_clearance'], success_data['min_ttc'],
                                c=success_data['trial'], cmap='viridis', alpha=0.7)
            ax4.set_xlabel('Minimum Clearance (m)')
            ax4.set_ylabel('Minimum Time-to-Collision (s)')
            ax4.set_title('Clearance vs Time-to-Collision')
            plt.colorbar(scatter, ax=ax4, label='Trial')
            
            # Add safety regions
            ax4.axhline(self.safety_thresholds['min_ttc']['critical'], color='red', 
                       linestyle='--', alpha=0.5)
            ax4.axvline(self.safety_thresholds['min_clearance']['critical'], color='red', 
                       linestyle='--', alpha=0.5)
        
        # 5. Self-Clearance Analysis
        ax5 = fig.add_subplot(gs[1, 1])
        self_clearance_metrics = ['min_self_clearance', 'avg_self_clearance']
        available_self = [m for m in self_clearance_metrics if m in success_data.columns]
        
        if available_self:
            trial_numbers = success_data['trial'].values
            for metric in available_self:
                values = success_data[metric].values
                ax5.plot(trial_numbers, values, marker='o', label=metric.replace('_', ' ').title())
            
            ax5.set_xlabel('Trial Number')
            ax5.set_ylabel('Self Clearance (m)')
            ax5.set_title('Self-Clearance Trends')
            ax5.legend()
            ax5.grid(True, alpha=0.3)
        
        # 6. Safety Score Dashboard
        ax6 = fig.add_subplot(gs[1, 2])
        
        # Calculate safety scores
        safety_scores = []
        safety_categories = []
        
        if 'min_clearance' in success_data.columns:
            clearance_score = (success_data['min_clearance'] >= 
                             self.safety_thresholds['min_clearance']['safe']).mean() * 100
            safety_scores.append(clearance_score)
            safety_categories.append('Clearance\nSafety')
        
        if 'min_ttc' in success_data.columns:
            ttc_score = (success_data['min_ttc'] >= 
                        self.safety_thresholds['min_ttc']['safe']).mean() * 100
            safety_scores.append(ttc_score)
            safety_categories.append('TTC\nSafety')
        
        if 'dynamic_feasibility' in success_data.columns:
            feasibility_score = success_data['dynamic_feasibility'].mean() * 100
            safety_scores.append(feasibility_score)
            safety_categories.append('Dynamic\nFeasibility')
        
        if safety_scores:
            # Create color map based on scores
            colors = ['red' if score < 50 else 'orange' if score < 80 else 'green' 
                     for score in safety_scores]
            
            bars = ax6.bar(safety_categories, safety_scores, color=colors, alpha=0.7)
            ax6.set_ylabel('Safety Score (%)')
            ax6.set_title('Safety Assessment Dashboard')
            ax6.set_ylim(0, 100)
            
            # Add score labels
            for bar, score in zip(bars, safety_scores):
                height = bar.get_height()
                ax6.text(bar.get_x() + bar.get_width()/2., height + 2,
                        f'{score:.1f}%', ha='center', va='bottom', fontweight='bold')
            
            # Add horizontal reference lines
            ax6.axhline(50, color='red', linestyle='--', alpha=0.5, label='Poor')
            ax6.axhline(80, color='orange', linestyle='--', alpha=0.5, label='Good')
        
        # 7. Clearance Path Analysis
        ax7 = fig.add_subplot(gs[2, :])
        if 'clearance_path_ratio' in success_data.columns:
            path_ratios = success_data['clearance_path_ratio']
            trials = success_data['trial']
            
            # Create stacked bar chart showing path safety
            safe_ratios = path_ratios
            unsafe_ratios = 1 - path_ratios
            
            ax7.bar(trials, safe_ratios, label='Safe Path Ratio', color='green', alpha=0.7)
            ax7.bar(trials, unsafe_ratios, bottom=safe_ratios, label='Unsafe Path Ratio', 
                   color='red', alpha=0.7)
            
            ax7.set_xlabel('Trial Number')
            ax7.set_ylabel('Path Ratio')
            ax7.set_title('Clearance-over-Path Analysis')
            ax7.legend()
            ax7.set_ylim(0, 1)
        
        # 8. Safety Summary Statistics
        ax8 = fig.add_subplot(gs[3, :])
        ax8.axis('off')
        
        # Create safety statistics table
        safety_summary = []
        key_safety_metrics = ['min_clearance', 'min_ttc', 'critical_violations', 'dynamic_feasibility']
        available_safety_key = [m for m in key_safety_metrics if m in success_data.columns]
        
        for metric in available_safety_key:
            values = success_data[metric]
            if metric == 'dynamic_feasibility':
                safety_summary.append([
                    metric.replace('_', ' ').title(),
                    f"{values.mean()*100:.1f}%",
                    f"{values.sum()}/{len(values)}",
                    "Boolean",
                    "1.0" if values.all() else "0.0",
                    f"{values.mean():.3f}"
                ])
            else:
                safety_summary.append([
                    metric.replace('_', ' ').title(),
                    f"{values.mean():.3f}",
                    f"{values.std():.3f}",
                    f"{values.min():.3f}",
                    f"{values.max():.3f}",
                    f"{values.median():.3f}"
                ])
        
        if safety_summary:
            table = ax8.table(cellText=safety_summary,
                            colLabels=['Safety Metric', 'Mean/Rate', 'Std Dev/Count', 'Min', 'Max', 'Median'],
                            cellLoc='center',
                            loc='center',
                            bbox=[0, 0, 1, 1])
            table.auto_set_font_size(False)
            table.set_fontsize(10)
            table.scale(1, 2)
            ax8.set_title('Safety Statistics Summary', pad=20)
        
        plt.suptitle('Trajectory Planning Safety Analysis', fontsize=16, fontweight='bold')
        return fig
    
    def create_quality_analysis(self):
        """Create comprehensive trajectory quality analysis visualizations."""
        if self.data is None:
            return
            
        success_data = self.data[self.data['success'] == 1]
        if len(success_data) == 0:
            return
            
        # Create figure with subplots
        fig = plt.figure(figsize=(20, 15))
        gs = GridSpec(4, 3, figure=fig, hspace=0.4, wspace=0.3)
        
        # 1. Manipulability Analysis
        ax1 = fig.add_subplot(gs[0, 0])
        manipulability_metrics = ['min_manipulability', 'avg_manipulability']
        available_manip = [m for m in manipulability_metrics if m in success_data.columns]
        
        if available_manip:
            for metric in available_manip:
                values = success_data[metric]
                ax1.hist(values, bins=15, alpha=0.6, label=metric.replace('_', ' ').title())
            
            # Add singularity threshold
            if 'critical' in self.safety_thresholds.get('min_manipulability', {}):
                ax1.axvline(self.safety_thresholds['min_manipulability']['critical'], 
                           color='red', linestyle='--', label='Singularity Threshold')
            
            ax1.set_xlabel('Manipulability Index')
            ax1.set_ylabel('Frequency')
            ax1.set_title('Manipulability Distribution')
            ax1.legend()
        
        # 2. Dynamic Feasibility Analysis
        ax2 = fig.add_subplot(gs[0, 1])
        torque_metrics = ['max_joint_torque', 'avg_joint_torque']
        available_torque = [m for m in torque_metrics if m in success_data.columns]
        
        if available_torque:
            for metric in available_torque:
                values = success_data[metric]
                ax2.hist(values, bins=15, alpha=0.6, label=metric.replace('_', ' ').title())
            
            # Add torque limits
            if 'safe' in self.safety_thresholds.get('max_joint_torque', {}):
                ax2.axvline(self.safety_thresholds['max_joint_torque']['safe'], 
                           color='green', linestyle='--', label='Safe Limit')
            if 'critical' in self.safety_thresholds.get('max_joint_torque', {}):
                ax2.axvline(self.safety_thresholds['max_joint_torque']['critical'], 
                           color='red', linestyle='--', label='Critical Limit')
            
            ax2.set_xlabel('Joint Torque (Nm)')
            ax2.set_ylabel('Frequency')
            ax2.set_title('Joint Torque Distribution')
            ax2.legend()
        
        # 3. Redundancy Utilization
        ax3 = fig.add_subplot(gs[0, 2])
        if 'redundancy_utilization' in success_data.columns:
            redundancy_values = success_data['redundancy_utilization']
            ax3.boxplot(redundancy_values)
            ax3.set_ylabel('Redundancy Utilization')
            ax3.set_title('Redundancy Utilization')
            ax3.text(1.1, redundancy_values.median(), 
                    f'Median: {redundancy_values.median():.3f}',
                    verticalalignment='center')
        
        # 4. Manipulability vs Redundancy
        ax4 = fig.add_subplot(gs[1, 0])
        if ('avg_manipulability' in success_data.columns and 
            'redundancy_utilization' in success_data.columns):
            scatter = ax4.scatter(success_data['avg_manipulability'], 
                                success_data['redundancy_utilization'],
                                c=success_data['trial'], cmap='viridis', alpha=0.7)
            ax4.set_xlabel('Average Manipulability')
            ax4.set_ylabel('Redundancy Utilization')
            ax4.set_title('Manipulability vs Redundancy')
            plt.colorbar(scatter, ax=ax4, label='Trial')
        
        # 5. Dynamic Feasibility Assessment
        ax5 = fig.add_subplot(gs[1, 1])
        if 'dynamic_feasibility' in success_data.columns:
            feasibility_counts = success_data['dynamic_feasibility'].value_counts()
            
            # Create pie chart
            labels = ['Feasible' if x == 1 else 'Infeasible' for x in feasibility_counts.index]
            colors = ['green' if 'Feasible' in label else 'red' for label in labels]
            
            ax5.pie(feasibility_counts.values, labels=labels, colors=colors, 
                   autopct='%1.1f%%', startangle=90)
            ax5.set_title('Dynamic Feasibility Assessment')
        
        # 6. Quality Score Radar Chart
        ax6 = fig.add_subplot(gs[1, 2], projection='polar')
        
        # Define quality metrics for radar chart
        quality_radar_metrics = ['min_manipulability', 'redundancy_utilization']
        available_radar = [m for m in quality_radar_metrics if m in success_data.columns]
        
        if available_radar and len(available_radar) >= 3:
            # Normalize metrics to 0-1 scale
            normalized_values = []
            metric_labels = []
            
            for metric in available_radar:
                values = success_data[metric]
                normalized = (values.mean() - values.min()) / (values.max() - values.min()) \
                           if values.max() > values.min() else 0.5
                normalized_values.append(normalized)
                metric_labels.append(metric.replace('_', ' ').title())
            
            # Add first point at the end to close the polygon
            normalized_values.append(normalized_values[0])
            metric_labels.append(metric_labels[0])
            
            # Create angles for each metric
            angles = np.linspace(0, 2 * np.pi, len(normalized_values), endpoint=True)
            
            ax6.plot(angles, normalized_values, 'o-', linewidth=2, label='Quality Profile')
            ax6.fill(angles, normalized_values, alpha=0.25)
            ax6.set_xticks(angles[:-1])
            ax6.set_xticklabels(metric_labels[:-1])
            ax6.set_ylim(0, 1)
            ax6.set_title('Quality Assessment Radar')
        else:
            ax6.text(0.5, 0.5, 'Insufficient Data\nfor Radar Chart', 
                    ha='center', va='center', transform=ax6.transAxes)
            ax6.set_title('Quality Assessment')
        
        # 7. Motion Smoothness Analysis
        ax7 = fig.add_subplot(gs[2, :])
        motion_metrics = ['velocity_variance', 'acceleration_variance', 'smoothness']
        available_motion = [m for m in motion_metrics if m in success_data.columns]
        
        if available_motion:
            # Create subplot for each motion metric
            n_metrics = len(available_motion)
            for i, metric in enumerate(available_motion):
                ax_sub = plt.subplot(2, n_metrics, n_metrics + i + 1)
                values = success_data[metric]
                trials = success_data['trial']
                
                ax_sub.plot(trials, values, marker='o', linewidth=2, 
                           label=metric.replace('_', ' ').title())
                ax_sub.set_xlabel('Trial Number')
                ax_sub.set_ylabel(metric.replace('_', ' ').title())
                ax_sub.grid(True, alpha=0.3)
                
                # Add trend line
                z = np.polyfit(trials, values, 1)
                p = np.poly1d(z)
                ax_sub.plot(trials, p(trials), "--", alpha=0.7, 
                           label=f'Trend: {z[0]:.6f}x+{z[1]:.6f}')
                ax_sub.legend()
            
            ax7.set_title('Motion Quality Trends Across Trials')
        
        # 8. Quality Summary Statistics
        ax8 = fig.add_subplot(gs[3, :])
        ax8.axis('off')
        
        # Create quality statistics table
        quality_summary = []
        key_quality_metrics = ['min_manipulability', 'avg_manipulability', 'redundancy_utilization', 
                              'max_joint_torque', 'velocity_variance', 'acceleration_variance']
        available_quality_key = [m for m in key_quality_metrics if m in success_data.columns]
        
        for metric in available_quality_key:
            values = success_data[metric]
            quality_summary.append([
                metric.replace('_', ' ').title(),
                f"{values.mean():.6f}",
                f"{values.std():.6f}",
                f"{values.min():.6f}",
                f"{values.max():.6f}",
                f"{values.median():.6f}"
            ])
        
        if quality_summary:
            table = ax8.table(cellText=quality_summary,
                            colLabels=['Quality Metric', 'Mean', 'Std Dev', 'Min', 'Max', 'Median'],
                            cellLoc='center',
                            loc='center',
                            bbox=[0, 0, 1, 1])
            table.auto_set_font_size(False)
            table.set_fontsize(10)
            table.scale(1, 2)
            ax8.set_title('Quality Statistics Summary', pad=20)
        
        plt.suptitle('Trajectory Planning Quality Analysis', fontsize=16, fontweight='bold')
        return fig
    
    def create_comprehensive_dashboard(self):
        """Create a comprehensive dashboard with key metrics."""
        if self.data is None:
            return
            
        success_data = self.data[self.data['success'] == 1]
        if len(success_data) == 0:
            return
            
        # Create figure
        fig = plt.figure(figsize=(24, 16))
        gs = GridSpec(3, 4, figure=fig, hspace=0.3, wspace=0.3)
        
        # 1. Success Rate and Overview
        ax1 = fig.add_subplot(gs[0, 0])
        success_rate = self.data['success'].mean() * 100
        
        # Create donut chart for success rate
        sizes = [success_rate, 100 - success_rate]
        colors = ['green', 'lightcoral']
        wedges, texts, autotexts = ax1.pie(sizes, colors=colors, autopct='%1.1f%%',
                                          startangle=90, counterclock=False)
        
        # Add circle in the center to make it a donut chart
        centre_circle = plt.Circle((0,0), 0.70, fc='white')
        ax1.add_artist(centre_circle)
        ax1.text(0, 0, f'{success_rate:.1f}%\nSuccess', ha='center', va='center', 
                fontsize=14, fontweight='bold')
        ax1.set_title('Overall Success Rate')
        
        # 2. Planning Time Distribution
        ax2 = fig.add_subplot(gs[0, 1])
        if 'planning_time_ms' in success_data.columns:
            planning_times = success_data['planning_time_ms'] / 1000
            ax2.hist(planning_times, bins=15, alpha=0.7, color='skyblue', edgecolor='black')
            ax2.axvline(planning_times.mean(), color='red', linestyle='--', 
                       label=f'Mean: {planning_times.mean():.1f}s')
            ax2.set_xlabel('Planning Time (s)')
            ax2.set_ylabel('Frequency')
            ax2.set_title('Planning Time Distribution')
            ax2.legend()
        
        # 3. Safety Score Summary
        ax3 = fig.add_subplot(gs[0, 2])
        safety_metrics_check = ['min_clearance', 'min_ttc', 'critical_violations']
        safety_scores = []
        safety_labels = []
        
        if 'min_clearance' in success_data.columns:
            clearance_score = (success_data['min_clearance'] >= 0.1).mean() * 100
            safety_scores.append(clearance_score)
            safety_labels.append('Clearance')
        
        if 'min_ttc' in success_data.columns:
            ttc_score = (success_data['min_ttc'] >= 1.0).mean() * 100
            safety_scores.append(ttc_score)
            safety_labels.append('TTC')
        
        if 'critical_violations' in success_data.columns:
            violation_score = (success_data['critical_violations'] == 0).mean() * 100
            safety_scores.append(violation_score)
            safety_labels.append('No Violations')
        
        if safety_scores:
            colors = ['green' if score >= 80 else 'orange' if score >= 60 else 'red' 
                     for score in safety_scores]
            bars = ax3.bar(safety_labels, safety_scores, color=colors, alpha=0.7)
            ax3.set_ylabel('Safety Score (%)')
            ax3.set_title('Safety Assessment')
            ax3.set_ylim(0, 100)
            
            for bar, score in zip(bars, safety_scores):
                height = bar.get_height()
                ax3.text(bar.get_x() + bar.get_width()/2., height + 2,
                        f'{score:.0f}%', ha='center', va='bottom')
        
        # 4. Quality Metrics Overview
        ax4 = fig.add_subplot(gs[0, 3])
        if 'min_manipulability' in success_data.columns:
            manipulability = success_data['min_manipulability']
            ax4.boxplot(manipulability, patch_artist=True, 
                       boxprops=dict(facecolor='lightblue', alpha=0.7))
            ax4.set_ylabel('Manipulability Index')
            ax4.set_title('Manipulability Distribution')
            ax4.text(1.1, manipulability.median(), 
                    f'Median: {manipulability.median():.3f}',
                    verticalalignment='center')
        
        # 5. Performance Trends
        ax5 = fig.add_subplot(gs[1, :2])
        if len(success_data) > 1:
            metrics_to_plot = ['planning_time_ms', 'trajectory_length', 'smoothness']
            available_metrics = [m for m in metrics_to_plot if m in success_data.columns]
            
            if available_metrics:
                trial_numbers = success_data['trial'].values
                
                for i, metric in enumerate(available_metrics):
                    values = success_data[metric].values
                    # Normalize to percentage of mean for comparison
                    normalized_values = (values / values.mean()) * 100
                    
                    ax5.plot(trial_numbers, normalized_values, marker='o', linewidth=2,
                            label=metric.replace('_', ' ').title())
                
                ax5.set_xlabel('Trial Number')
                ax5.set_ylabel('% of Mean Value')
                ax5.set_title('Performance Metrics Trends (Normalized)')
                ax5.legend()
                ax5.grid(True, alpha=0.3)
                ax5.axhline(100, color='black', linestyle='-', alpha=0.5, label='Mean')
        
        # 6. Clearance vs TTC Safety Map
        ax6 = fig.add_subplot(gs[1, 2:])
        if ('min_clearance' in success_data.columns and 
            'min_ttc' in success_data.columns):
            
            clearance = success_data['min_clearance']
            ttc = success_data['min_ttc']
            
            # Create safety regions
            ax6.axhspan(0, 1.0, 0, 0.1, alpha=0.3, color='red', label='High Risk')
            ax6.axhspan(1.0, 3.0, 0.1, 0.15, alpha=0.3, color='orange', label='Medium Risk')
            ax6.axhspan(3.0, ax6.get_ylim()[1], 0.15, 1, alpha=0.3, color='green', label='Low Risk')
            
            scatter = ax6.scatter(clearance, ttc, c=success_data['trial'], 
                                cmap='viridis', s=60, alpha=0.8, edgecolors='black')
            
            ax6.set_xlabel('Minimum Clearance (m)')
            ax6.set_ylabel('Minimum Time-to-Collision (s)')
            ax6.set_title('Safety Assessment Map')
            plt.colorbar(scatter, ax=ax6, label='Trial Number')
            
            # Add threshold lines
            ax6.axhline(1.0, color='red', linestyle='--', alpha=0.7)
            ax6.axhline(3.0, color='green', linestyle='--', alpha=0.7)
            ax6.axvline(0.05, color='red', linestyle='--', alpha=0.7)
            ax6.axvline(0.15, color='green', linestyle='--', alpha=0.7)
        
        # 7. Summary Statistics Table
        ax7 = fig.add_subplot(gs[2, :])
        ax7.axis('off')
        
        # Create comprehensive summary table
        summary_data = []
        
        # Key metrics summary
        key_metrics = {
            'Planning Time (s)': 'planning_time_ms',
            'Trajectory Length': 'trajectory_length',
            'Smoothness': 'smoothness',
            'Min Clearance (m)': 'min_clearance',
            'Min TTC (s)': 'min_ttc',
            'Min Manipulability': 'min_manipulability',
            'Max Torque (Nm)': 'max_joint_torque',
            'Execution Time (s)': 'execution_time'
        }
        
        for display_name, metric in key_metrics.items():
            if metric in success_data.columns:
                values = success_data[metric]
                if metric == 'planning_time_ms':
                    values = values / 1000  # Convert to seconds
                
                summary_data.append([
                    display_name,
                    f"{values.mean():.3f}",
                    f"{values.std():.3f}",
                    f"{values.min():.3f}",
                    f"{values.max():.3f}",
                    f"{values.median():.3f}",
                    "âœ…" if values.mean() < values.quantile(0.75) else "âš ï¸"
                ])
        
        if summary_data:
            table = ax7.table(cellText=summary_data,
                            colLabels=['Metric', 'Mean', 'Std Dev', 'Min', 'Max', 'Median', 'Status'],
                            cellLoc='center',
                            loc='center',
                            bbox=[0, 0, 1, 1])
            table.auto_set_font_size(False)
            table.set_fontsize(11)
            table.scale(1, 2.5)
            
            # Color code the table
            for i in range(len(summary_data)):
                if summary_data[i][6] == "âœ…":
                    table[(i+1, 6)].set_facecolor('#90EE90')  # Light green
                else:
                    table[(i+1, 6)].set_facecolor('#FFB6C1')  # Light red
            
            ax7.set_title('Comprehensive Performance Summary', pad=20, fontsize=14, fontweight='bold')
        
        plt.suptitle('Trajectory Planning Evaluation Dashboard', fontsize=18, fontweight='bold')
        return fig
    
    def export_analysis_report(self, output_dir=None):
        """Export comprehensive analysis report with all visualizations."""
        if output_dir is None:
            output_dir = self.data_file.parent / f"analysis_report_{self.analysis_timestamp}"
        else:
            output_dir = Path(output_dir)
        
        output_dir.mkdir(exist_ok=True)
        
        print(f"ðŸ“ Exporting analysis report to: {output_dir}")
        
        # Generate and save all visualizations
        figures = {}
        
        try:
            # Performance Analysis
            print("ðŸ”„ Generating performance analysis...")
            figures['performance'] = self.create_performance_analysis()
            if figures['performance']:
                figures['performance'].savefig(output_dir / 'performance_analysis.png', 
                                             dpi=300, bbox_inches='tight')
                plt.close(figures['performance'])
            
            # Safety Analysis
            print("ðŸ”„ Generating safety analysis...")
            figures['safety'] = self.create_safety_analysis()
            if figures['safety']:
                figures['safety'].savefig(output_dir / 'safety_analysis.png', 
                                        dpi=300, bbox_inches='tight')
                plt.close(figures['safety'])
            
            # Quality Analysis
            print("ðŸ”„ Generating quality analysis...")
            figures['quality'] = self.create_quality_analysis()
            if figures['quality']:
                figures['quality'].savefig(output_dir / 'quality_analysis.png', 
                                         dpi=300, bbox_inches='tight')
                plt.close(figures['quality'])
            
            # Comprehensive Dashboard
            print("ðŸ”„ Generating comprehensive dashboard...")
            figures['dashboard'] = self.create_comprehensive_dashboard()
            if figures['dashboard']:
                figures['dashboard'].savefig(output_dir / 'comprehensive_dashboard.png', 
                                           dpi=300, bbox_inches='tight')
                plt.close(figures['dashboard'])
            
            # Export statistics summary
            print("ðŸ”„ Exporting statistics summary...")
            with open(output_dir / 'statistics_summary.json', 'w') as f:
                json.dump(self.stats_summary, f, indent=2)
            
            # Export processed data
            print("ðŸ”„ Exporting processed data...")
            if self.data is not None:
                success_data = self.data[self.data['success'] == 1]
                success_data.to_csv(output_dir / 'successful_trials_data.csv', index=False)
                self.data.to_csv(output_dir / 'complete_data.csv', index=False)
            
            # Generate text report
            print("ðŸ”„ Generating text report...")
            self._generate_text_report(output_dir)
            
            print(f"âœ… Analysis report exported successfully!")
            print(f"ðŸ“Š Generated files:")
            for file in output_dir.iterdir():
                print(f"   - {file.name}")
            
            return output_dir
            
        except Exception as e:
            print(f"âŒ Error exporting analysis report: {e}")
            return None
    
    def _generate_text_report(self, output_dir):
        """Generate comprehensive text report."""
        report_path = output_dir / 'analysis_report.txt'
        
        with open(report_path, 'w') as f:
            f.write("="*80 + "\n")
            f.write("TRAJECTORY PLANNING EVALUATION ANALYSIS REPORT\n")
            f.write("="*80 + "\n\n")
            
            f.write(f"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"Data File: {self.data_file}\n")
            f.write(f"Analysis ID: {self.analysis_timestamp}\n\n")
            
            # Overview Section
            f.write("EXECUTIVE SUMMARY\n")
            f.write("-"*40 + "\n")
            if 'overview' in self.stats_summary:
                overview = self.stats_summary['overview']
                f.write(f"Total Trials: {overview['total_trials']}\n")
                f.write(f"Successful Trials: {overview['successful_trials']}\n")
                f.write(f"Success Rate: {overview['success_rate']:.1f}%\n")
                f.write(f"Algorithms: {', '.join(overview['algorithms'])}\n\n")
            
            # Performance Section
            if 'performance' in self.stats_summary:
                f.write("PERFORMANCE METRICS\n")
                f.write("-"*40 + "\n")
                for metric, stats in self.stats_summary['performance'].items():
                    f.write(f"{metric.replace('_', ' ').title()}:\n")
                    f.write(f"  Mean: {stats['mean']:.3f}\n")
                    f.write(f"  Std Dev: {stats['std']:.3f}\n")
                    f.write(f"  Range: {stats['min']:.3f} - {stats['max']:.3f}\n\n")
            
            # Safety Section
            if 'safety' in self.stats_summary:
                f.write("SAFETY METRICS\n")
                f.write("-"*40 + "\n")
                for metric, stats in self.stats_summary['safety'].items():
                    f.write(f"{metric.replace('_', ' ').title()}:\n")
                    f.write(f"  Mean: {stats['mean']:.3f}\n")
                    f.write(f"  Std Dev: {stats['std']:.3f}\n")
                    f.write(f"  Range: {stats['min']:.3f} - {stats['max']:.3f}\n\n")
            
            # Quality Section
            if 'quality' in self.stats_summary:
                f.write("QUALITY METRICS\n")
                f.write("-"*40 + "\n")
                for metric, stats in self.stats_summary['quality'].items():
                    f.write(f"{metric.replace('_', ' ').title()}:\n")
                    if 'feasible_ratio' in stats:
                        f.write(f"  Feasible Ratio: {stats['feasible_ratio']*100:.1f}%\n")
                        f.write(f"  Feasible Trials: {stats['total_feasible']}/{stats['total_trials']}\n\n")
                    else:
                        f.write(f"  Mean: {stats['mean']:.6f}\n")
                        f.write(f"  Std Dev: {stats['std']:.6f}\n")
                        f.write(f"  Range: {stats['min']:.6f} - {stats['max']:.6f}\n\n")
            
            f.write("="*80 + "\n")
            f.write("End of Report\n")

def main():
    """Main function for command-line usage."""
    parser = argparse.ArgumentParser(description='Trajectory Planning Analysis and Visualization')
    parser.add_argument('data_file', help='Path to CSV data file')
    parser.add_argument('--output', '-o', help='Output directory for analysis results')
    parser.add_argument('--show', '-s', action='store_true', help='Show plots interactively')
    parser.add_argument('--export', '-e', action='store_true', help='Export analysis report')
    
    args = parser.parse_args()
    
    # Initialize analyzer
    analyzer = TrajectoryPlanningAnalyzer(args.data_file)
    
    # Load data
    if not analyzer.load_data():
        return 1
    
    # Generate statistics
    analyzer.generate_summary_statistics()
    
    # Show plots if requested
    if args.show:
        print("ðŸ”„ Generating interactive visualizations...")
        
        # Create all analyses
        performance_fig = analyzer.create_performance_analysis()
        safety_fig = analyzer.create_safety_analysis()
        quality_fig = analyzer.create_quality_analysis()
        dashboard_fig = analyzer.create_comprehensive_dashboard()
        
        # Show plots
        if performance_fig:
            plt.show()
        if safety_fig:
            plt.show()
        if quality_fig:
            plt.show()
        if dashboard_fig:
            plt.show()
    
    # Export report if requested
    if args.export:
        output_dir = analyzer.export_analysis_report(args.output)
        if output_dir:
            print(f"ðŸ“ Analysis report exported to: {output_dir}")
    
    return 0

if __name__ == "__main__":
    exit(main())
