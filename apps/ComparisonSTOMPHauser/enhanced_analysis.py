#!/usr/bin/env python3
"""
Enhanced STOMP vs Hauser Trajectory Analysis
============================================

This script provides comprehensive analysis and visualization of trajectory planning
comparison data generated by the C++ comparison tool.

Features:
- Computation time analysis with boxplots
- Trajectory execution time comparison
- Smoothness metrics (acceleration, jerk)
- Clearance analysis (minimum, average, variance)
- Joint angle variance across multiple trials
- Statistical significance testing
- Publication-quality visualizations

Usage:
    python3 enhanced_analysis.py <results_csv> <trajectories_dir>
"""

import sys
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import glob
from scipy import stats
from scipy.signal import savgol_filter
import subprocess
import tempfile
import warnings
warnings.filterwarnings('ignore')

# Set up plotting style to match ComparisonIK
plt.style.use('default')
sns.set_palette(["#1f77b4", "#ff7f0e"])  # Default matplotlib colors: blue, orange

class TrajectoryAnalyzer:
    def __init__(self, results_file, trajectories_dir):
        """Initialize analyzer with data files."""
        self.results_file = results_file
        self.trajectories_dir = trajectories_dir
        self.results_df = None
        self.trajectory_data = {}
        self.trajectory_metrics_df = pd.DataFrame()
        self.plots_dir = "plots"
        
        # Create plots directory
        Path(self.plots_dir).mkdir(exist_ok=True)
        
        print(f"üìä Enhanced Trajectory Analysis")
        print(f"Results file: {results_file}")
        print(f"Trajectories dir: {trajectories_dir}")
        
    def load_data(self):
        """Load results and trajectory data."""
        print("\nüìÇ Loading data...")
        
        # Load results CSV
        if not os.path.exists(self.results_file):
            raise FileNotFoundError(f"Results file not found: {self.results_file}")
            
        self.results_df = pd.read_csv(self.results_file)
        print(f"‚úì Loaded {len(self.results_df)} trial results")
        
        # Load trajectory files
        if os.path.exists(self.trajectories_dir):
            traj_files = glob.glob(f"{self.trajectories_dir}/*.csv")
            for traj_file in traj_files:
                filename = os.path.basename(traj_file)
                try:
                    df = pd.read_csv(traj_file)
                    self.trajectory_data[filename] = df
                except Exception as e:
                    print(f"‚ö†Ô∏è  Failed to load {filename}: {e}")
            
            print(f"‚úì Loaded {len(self.trajectory_data)} trajectory files")
        else:
            print(f"‚ö†Ô∏è  Trajectories directory not found: {self.trajectories_dir}")
    
    def compute_trajectory_metrics(self):
        """Compute comprehensive trajectory metrics."""
        print("\nüìê Computing trajectory metrics...")
        
        metrics_list = []
        
        for filename, traj_df in self.trajectory_data.items():
            try:
                # Parse filename to extract metadata
                parts = filename.replace('.csv', '').split('_')
                algorithm = parts[0]
                pose_name = parts[1]
                trial_info = parts[2]  # e.g., "trial1"
                trial_number = int(trial_info.replace('trial', ''))
                
                metrics = self.calculate_single_trajectory_metrics(traj_df)
                metrics.update({
                    'algorithm': algorithm,
                    'pose_name': pose_name,
                    'trial_number': trial_number,
                    'filename': filename
                })
                
                metrics_list.append(metrics)
                
            except Exception as e:
                print(f"‚ö†Ô∏è  Error processing {filename}: {e}")
        
        if metrics_list:
            self.trajectory_metrics_df = pd.DataFrame(metrics_list)
            print(f"‚úì Computed metrics for {len(metrics_list)} trajectories")
        else:
            print("‚ö†Ô∏è  No trajectory metrics computed")
            self.trajectory_metrics_df = pd.DataFrame()
    
    def calculate_single_trajectory_metrics(self, traj_df):
        """Calculate metrics for a single trajectory."""
        metrics = {}
        
        # Basic trajectory properties
        metrics['trajectory_length'] = len(traj_df)
        metrics['execution_time'] = traj_df['time'].iloc[-1] if len(traj_df) > 0 else 0
        
        # Joint space path length
        joint_cols = ['j0', 'j1', 'j2', 'j3', 'j4', 'j5', 'j6']
        if all(col in traj_df.columns for col in joint_cols):
            joint_diffs = traj_df[joint_cols].diff().dropna()
            path_segments = np.linalg.norm(joint_diffs.values, axis=1)
            metrics['path_length'] = np.sum(path_segments)
            metrics['avg_segment_length'] = np.mean(path_segments)
        
        # Enhanced Smoothness metrics
        vel_cols = ['vel0', 'vel1', 'vel2', 'vel3', 'vel4', 'vel5', 'vel6']
        acc_cols = ['acc0', 'acc1', 'acc2', 'acc3', 'acc4', 'acc5', 'acc6']
        
        if all(col in traj_df.columns for col in vel_cols):
            velocities = traj_df[vel_cols].values
            metrics['max_velocity'] = np.max(np.abs(velocities))
            metrics['avg_velocity'] = np.mean(np.abs(velocities))
            metrics['velocity_variance'] = np.var(velocities)
            
            # Velocity smoothness: sum of squared velocity changes
            vel_changes = np.diff(velocities, axis=0)
            metrics['velocity_smoothness'] = np.sum(vel_changes**2)
        
        if all(col in traj_df.columns for col in acc_cols):
            accelerations = traj_df[acc_cols].values
            metrics['max_acceleration'] = np.max(np.abs(accelerations))
            metrics['avg_acceleration'] = np.mean(np.abs(accelerations))
            metrics['acceleration_variance'] = np.var(accelerations)
            
            # Acceleration smoothness: sum of squared acceleration changes
            acc_changes = np.diff(accelerations, axis=0)
            metrics['acceleration_smoothness'] = np.sum(acc_changes**2)
            
            # Compute jerk (derivative of acceleration)
            if len(accelerations) > 1:
                dt_values = np.diff(traj_df['time'].values)
                dt_values = dt_values[dt_values > 0]  # Remove zero time steps
                if len(dt_values) > 0:
                    avg_dt = np.mean(dt_values)
                    jerk = np.diff(accelerations, axis=0) / avg_dt
                    metrics['max_jerk'] = np.max(np.abs(jerk))
                    metrics['avg_jerk'] = np.mean(np.abs(jerk))
                    
                    # Jerk-based smoothness score (lower jerk = higher smoothness)
                    metrics['jerk_smoothness'] = np.sum(jerk**2)  # Sum of squared jerks
                    metrics['smoothness_score'] = 1.0 / (1.0 + metrics['avg_jerk'])
        
        # Joint range usage
        if all(col in traj_df.columns for col in joint_cols):
            joint_ranges = traj_df[joint_cols].max() - traj_df[joint_cols].min()
            # Normalize by typical joint limits (Franka Panda limits in radians)
            joint_limits = [2.8, 1.76, 2.8, 3.07, 2.8, 3.75, 2.8]
            normalized_ranges = joint_ranges / joint_limits
            metrics['joint_range_usage'] = np.mean(normalized_ranges)
            metrics['max_joint_range_usage'] = np.max(normalized_ranges)
        
        return metrics
    
    def calculate_clearance_metrics(self):
        """Calculate clearance metrics using C++ clearance calculator."""
        print("\nüõ°Ô∏è  Calculating clearance metrics...")
        
        # Check if clearance calculator exists
        clearance_calculator_path = "/Users/joris/Uni/MA/Code/PathPlanner_US_wip/build/apps/ComparisonSTOMPHauser/ClearanceCalculator"
        if not os.path.exists(clearance_calculator_path):
            print("‚ö†Ô∏è  ClearanceCalculator not found. Skipping clearance analysis.")
            return
        
        clearance_data = []
        
        for filename, traj_df in self.trajectory_data.items():
            try:
                # Parse filename to extract metadata
                parts = filename.replace('.csv', '').split('_')
                if len(parts) < 3:
                    print(f"‚ö†Ô∏è  Unexpected filename format: {filename}")
                    continue
                    
                algorithm = parts[0]
                pose_name = parts[1]
                trial_info = parts[2]
                
                # Handle both "trial1" and "trial1_trajectory" formats
                if 'trial' in trial_info:
                    trial_number = int(trial_info.replace('trial', '').replace('trajectory', ''))
                else:
                    trial_number = 1
                
                # Full path to trajectory file
                traj_file = os.path.join(self.trajectories_dir, filename)
                
                # Create temporary output file
                with tempfile.NamedTemporaryFile(mode='w', suffix='.csv', delete=False) as temp_out:
                    temp_output = temp_out.name
                
                # Run clearance calculator
                try:
                    result = subprocess.run([
                        clearance_calculator_path, 
                        traj_file, 
                        temp_output
                    ], capture_output=True, text=True, timeout=60)
                    
                    if result.returncode == 0:
                        # Read clearance metrics
                        if os.path.exists(temp_output):
                            clearance_df = pd.read_csv(temp_output)
                            if len(clearance_df) > 0:
                                clearance_metrics = clearance_df.iloc[0].to_dict()
                                clearance_metrics.update({
                                    'algorithm': algorithm,
                                    'pose_name': pose_name,
                                    'trial_number': trial_number,
                                    'filename': filename
                                })
                                clearance_data.append(clearance_metrics)
                                print(f"‚úì Calculated clearance for {filename}")
                    else:
                        print(f"‚ö†Ô∏è  Clearance calculation failed for {filename}: {result.stderr}")
                
                except subprocess.TimeoutExpired:
                    print(f"‚ö†Ô∏è  Clearance calculation timed out for {filename}")
                except Exception as e:
                    print(f"‚ö†Ô∏è  Error calculating clearance for {filename}: {e}")
                
                # Clean up temporary file
                try:
                    os.unlink(temp_output)
                except:
                    pass
                    
            except Exception as e:
                print(f"‚ö†Ô∏è  Error processing {filename}: {e}")
        
        if clearance_data:
            self.clearance_df = pd.DataFrame(clearance_data)
            print(f"‚úì Calculated clearance metrics for {len(clearance_data)} trajectories")
            
            # Create clearance analysis plots
            self.analyze_clearance_metrics()
        else:
            print("‚ö†Ô∏è  No clearance metrics calculated")
            self.clearance_df = pd.DataFrame()
    
    def analyze_clearance_metrics(self):
        """Analyze and visualize clearance metrics."""
        if self.clearance_df.empty:
            print("‚ö†Ô∏è  No clearance data available for analysis")
            return
        
        print("\nüìä Analyzing clearance metrics...")
        
        # Create clearance analysis plots
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        fig.suptitle('Robot Arm Clearance Analysis', fontsize=16, fontweight='bold')
        
        # 1. Minimum clearance comparison
        sns.boxplot(data=self.clearance_df, x='algorithm', y='min_clearance', ax=axes[0, 0])
        axes[0, 0].set_title('Minimum Clearance by Algorithm')
        axes[0, 0].set_ylabel('Min Clearance (m)')
        axes[0, 0].axhline(y=0.05, color='red', linestyle='--', alpha=0.7, label='5cm safety threshold')
        axes[0, 0].legend()
        
        # 2. Average clearance comparison
        sns.boxplot(data=self.clearance_df, x='algorithm', y='avg_clearance', ax=axes[0, 1])
        axes[0, 1].set_title('Average Clearance by Algorithm')
        axes[0, 1].set_ylabel('Avg Clearance (m)')
        
        # 3. Clearance variance comparison
        sns.boxplot(data=self.clearance_df, x='algorithm', y='clearance_variance', ax=axes[1, 0])
        axes[1, 0].set_title('Clearance Variance by Algorithm')
        axes[1, 0].set_ylabel('Clearance Variance (m¬≤)')
        
        # 4. Clearance by pose
        sns.boxplot(data=self.clearance_df, x='pose_name', y='min_clearance', hue='algorithm', ax=axes[1, 1])
        axes[1, 1].set_title('Minimum Clearance by Pose')
        axes[1, 1].set_ylabel('Min Clearance (m)')
        axes[1, 1].tick_params(axis='x', rotation=45)
        axes[1, 1].axhline(y=0.05, color='red', linestyle='--', alpha=0.7)
        
        plt.tight_layout()
        plt.savefig(f'{self.plots_dir}/clearance_analysis.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        # Print clearance summary
        print("\nüìä Clearance Summary:")
        for algorithm in self.clearance_df['algorithm'].unique():
            alg_data = self.clearance_df[self.clearance_df['algorithm'] == algorithm]
            print(f"  {algorithm.upper()}:")
            
            min_clearances = alg_data['min_clearance']
            avg_clearances = alg_data['avg_clearance']
            variances = alg_data['clearance_variance']
            
            print(f"    Min Clearance: {min_clearances.mean():.4f} ¬± {min_clearances.std():.4f} m")
            print(f"    Avg Clearance: {avg_clearances.mean():.4f} ¬± {avg_clearances.std():.4f} m")
            print(f"    Clearance Variance: {variances.mean():.6f} ¬± {variances.std():.6f} m¬≤")
            
            # Safety analysis
            unsafe_count = len(min_clearances[min_clearances < 0.05])
            if unsafe_count > 0:
                print(f"    ‚ö†Ô∏è  {unsafe_count}/{len(min_clearances)} trajectories below 5cm safety threshold")
            else:
                print(f"    ‚úì All trajectories above 5cm safety threshold")
            print()
    
    def analyze_computation_times(self):
        """Analyze and visualize computation times with enhanced boxplots."""
        print("\n‚è±Ô∏è  Analyzing computation times...")
        
        # Filter successful trials
        success_df = self.results_df[self.results_df['success'] == 1].copy()
        
        if len(success_df) == 0:
            print("‚ö†Ô∏è  No successful trials to analyze")
            return
        
        # Create enhanced computation time analysis
        fig, axes = plt.subplots(2, 3, figsize=(20, 12))
        fig.suptitle('Computation Time Analysis', fontsize=16, fontweight='bold')
        
        # 1. Main boxplot by algorithm
        sns.boxplot(data=success_df, x='algorithm', y='planning_time_ms', ax=axes[0, 0])
        axes[0, 0].set_title('Planning Time Distribution by Algorithm')
        axes[0, 0].set_ylabel('Planning Time (ms)')
        
        # 2. Boxplot by pose and algorithm
        sns.boxplot(data=success_df, x='pose_name', y='planning_time_ms', hue='algorithm', ax=axes[0, 1])
        axes[0, 1].set_title('Planning Time by Pose')
        axes[0, 1].set_ylabel('Planning Time (ms)')
        axes[0, 1].tick_params(axis='x', rotation=45)
        
        # 3. Violin plot for distribution details
        sns.violinplot(data=success_df, x='algorithm', y='planning_time_ms', ax=axes[0, 2])
        axes[0, 2].set_title('Planning Time Distribution (Violin Plot)')
        axes[0, 2].set_ylabel('Planning Time (ms)')
        
        # 4. Histogram comparison
        for algorithm in success_df['algorithm'].unique():
            alg_data = success_df[success_df['algorithm'] == algorithm]['planning_time_ms']
            axes[1, 0].hist(alg_data, alpha=0.7, label=algorithm, bins=20)
        axes[1, 0].set_title('Planning Time Distribution')
        axes[1, 0].set_xlabel('Planning Time (ms)')
        axes[1, 0].set_ylabel('Frequency')
        axes[1, 0].legend()
        
        # 5. Success rate by algorithm
        success_rates = self.results_df.groupby('algorithm')['success'].agg(['count', 'sum']).reset_index()
        success_rates['success_rate'] = success_rates['sum'] / success_rates['count'] * 100
        
        bars = axes[1, 1].bar(success_rates['algorithm'], success_rates['success_rate'])
        axes[1, 1].set_title('Success Rate by Algorithm')
        axes[1, 1].set_ylabel('Success Rate (%)')
        axes[1, 1].set_ylim(0, 100)
        
        # Add value labels on bars
        for bar, rate in zip(bars, success_rates['success_rate']):
            axes[1, 1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, 
                           f'{rate:.1f}%', ha='center', va='bottom')
        
        # 6. Planning time vs trajectory points scatter
        sns.scatterplot(data=success_df, x='trajectory_points', y='planning_time_ms', 
                       hue='algorithm', ax=axes[1, 2])
        axes[1, 2].set_title('Planning Time vs Trajectory Complexity')
        axes[1, 2].set_xlabel('Trajectory Points')
        axes[1, 2].set_ylabel('Planning Time (ms)')
        
        plt.tight_layout()
        plt.savefig(f'{self.plots_dir}/computation_time_analysis.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        # Statistical analysis
        self.perform_statistical_tests(success_df)
    
    def analyze_trajectory_execution_times(self):
        """Analyze trajectory execution times with boxplots."""
        print("\nüïí Analyzing trajectory execution times...")
        
        if self.trajectory_metrics_df.empty:
            print("‚ö†Ô∏è  No trajectory metrics available for execution time analysis")
            return
        
        # Create execution time analysis
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle('Trajectory Execution Time Analysis', fontsize=16, fontweight='bold')
        
        # 1. Main boxplot by algorithm
        sns.boxplot(data=self.trajectory_metrics_df, x='algorithm', y='execution_time', ax=axes[0, 0])
        axes[0, 0].set_title('Execution Time Distribution by Algorithm')
        axes[0, 0].set_ylabel('Execution Time (s)')
        
        # 2. Boxplot by pose and algorithm
        sns.boxplot(data=self.trajectory_metrics_df, x='pose_name', y='execution_time', 
                   hue='algorithm', ax=axes[0, 1])
        axes[0, 1].set_title('Execution Time by Pose')
        axes[0, 1].set_ylabel('Execution Time (s)')
        axes[0, 1].tick_params(axis='x', rotation=45)
        
        # 3. Violin plot for distribution details
        sns.violinplot(data=self.trajectory_metrics_df, x='algorithm', y='execution_time', ax=axes[1, 0])
        axes[1, 0].set_title('Execution Time Distribution (Violin Plot)')
        axes[1, 0].set_ylabel('Execution Time (s)')
        
        # 4. Execution time vs path length
        sns.scatterplot(data=self.trajectory_metrics_df, x='path_length', y='execution_time',
                       hue='algorithm', ax=axes[1, 1])
        axes[1, 1].set_title('Execution Time vs Path Length')
        axes[1, 1].set_xlabel('Path Length (rad)')
        axes[1, 1].set_ylabel('Execution Time (s)')
        
        plt.tight_layout()
        plt.savefig(f'{self.plots_dir}/execution_time_analysis.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        # Print summary statistics
        print("\nüìä Execution Time Summary:")
        for algorithm in self.trajectory_metrics_df['algorithm'].unique():
            alg_data = self.trajectory_metrics_df[self.trajectory_metrics_df['algorithm'] == algorithm]
            exec_times = alg_data['execution_time']
            print(f"  {algorithm.upper()}:")
            print(f"    Mean: {exec_times.mean():.3f}s ¬± {exec_times.std():.3f}s")
            print(f"    Median: {exec_times.median():.3f}s")
            print(f"    Range: {exec_times.min():.3f}s - {exec_times.max():.3f}s")
    
    def perform_statistical_tests(self, success_df):
        """Perform statistical tests on computation times."""
        print("\nüìä Statistical Analysis:")
        
        algorithms = success_df['algorithm'].unique()
        if len(algorithms) < 2:
            print("‚ö†Ô∏è  Need at least 2 algorithms for statistical comparison")
            return
        
        # Group data by algorithm
        groups = [success_df[success_df['algorithm'] == alg]['planning_time_ms'].values 
                 for alg in algorithms]
        
        # Perform t-test or Mann-Whitney U test
        if len(groups) == 2:
            # Check normality
            _, p_norm1 = stats.shapiro(groups[0]) if len(groups[0]) > 3 else (None, 0.05)
            _, p_norm2 = stats.shapiro(groups[1]) if len(groups[1]) > 3 else (None, 0.05)
            
            if p_norm1 > 0.05 and p_norm2 > 0.05:
                # Use t-test for normal data
                statistic, p_value = stats.ttest_ind(groups[0], groups[1])
                test_name = "Independent t-test"
            else:
                # Use Mann-Whitney U for non-normal data
                statistic, p_value = stats.mannwhitneyu(groups[0], groups[1], alternative='two-sided')
                test_name = "Mann-Whitney U test"
            
            print(f"{test_name}: statistic={statistic:.3f}, p-value={p_value:.3f}")
            
            if p_value < 0.05:
                print(f"‚úì Significant difference between {algorithms[0]} and {algorithms[1]} (p < 0.05)")
            else:
                print(f"‚úó No significant difference between {algorithms[0]} and {algorithms[1]} (p ‚â• 0.05)")
        
        # Summary statistics
        print("\nSummary Statistics:")
        for alg in algorithms:
            alg_data = success_df[success_df['algorithm'] == alg]['planning_time_ms']
            print(f"{alg}:")
            print(f"  Mean: {alg_data.mean():.1f} ms")
            print(f"  Median: {alg_data.median():.1f} ms")
            print(f"  Std: {alg_data.std():.1f} ms")
            print(f"  Min: {alg_data.min():.1f} ms")
            print(f"  Max: {alg_data.max():.1f} ms")
    
    def analyze_trajectory_quality(self):
        """Analyze trajectory quality metrics with focus on smoothness."""
        if self.trajectory_metrics_df.empty:
            print("‚ö†Ô∏è  No trajectory metrics available for quality analysis")
            return
        
        print("\nüìà Analyzing trajectory quality and smoothness...")
        
        # Create comprehensive smoothness analysis
        fig, axes = plt.subplots(3, 3, figsize=(20, 18))
        fig.suptitle('Trajectory Quality and Smoothness Analysis', fontsize=16, fontweight='bold')
        
        # 1. Smoothness score comparison
        if 'smoothness_score' in self.trajectory_metrics_df.columns:
            sns.boxplot(data=self.trajectory_metrics_df, x='algorithm', y='smoothness_score', ax=axes[0, 0])
            axes[0, 0].set_title('Smoothness Score (Higher = Smoother)')
            axes[0, 0].set_ylabel('Smoothness Score')
        
        # 2. Jerk-based smoothness
        if 'jerk_smoothness' in self.trajectory_metrics_df.columns:
            sns.boxplot(data=self.trajectory_metrics_df, x='algorithm', y='jerk_smoothness', ax=axes[0, 1])
            axes[0, 1].set_title('Jerk Smoothness (Lower = Smoother)')
            axes[0, 1].set_ylabel('Sum of Squared Jerks')
            axes[0, 1].set_yscale('log')  # Log scale for better visualization
        
        # 3. Acceleration smoothness
        if 'acceleration_smoothness' in self.trajectory_metrics_df.columns:
            sns.boxplot(data=self.trajectory_metrics_df, x='algorithm', y='acceleration_smoothness', ax=axes[0, 2])
            axes[0, 2].set_title('Acceleration Smoothness (Lower = Smoother)')
            axes[0, 2].set_ylabel('Sum of Squared Acc. Changes')
            axes[0, 2].set_yscale('log')
        
        # 4. Maximum jerk
        if 'max_jerk' in self.trajectory_metrics_df.columns:
            sns.boxplot(data=self.trajectory_metrics_df, x='algorithm', y='max_jerk', ax=axes[1, 0])
            axes[1, 0].set_title('Maximum Jerk')
            axes[1, 0].set_ylabel('Max Jerk (rad/s¬≥)')
        
        # 5. Average jerk
        if 'avg_jerk' in self.trajectory_metrics_df.columns:
            sns.boxplot(data=self.trajectory_metrics_df, x='algorithm', y='avg_jerk', ax=axes[1, 1])
            axes[1, 1].set_title('Average Jerk')
            axes[1, 1].set_ylabel('Avg Jerk (rad/s¬≥)')
        
        # 6. Maximum acceleration
        if 'max_acceleration' in self.trajectory_metrics_df.columns:
            sns.boxplot(data=self.trajectory_metrics_df, x='algorithm', y='max_acceleration', ax=axes[1, 2])
            axes[1, 2].set_title('Maximum Acceleration')
            axes[1, 2].set_ylabel('Max Acceleration (rad/s¬≤)')
        
        # 7. Path length
        if 'path_length' in self.trajectory_metrics_df.columns:
            sns.boxplot(data=self.trajectory_metrics_df, x='algorithm', y='path_length', ax=axes[2, 0])
            axes[2, 0].set_title('Path Length')
            axes[2, 0].set_ylabel('Path Length (rad)')
        
        # 8. Joint range usage
        if 'joint_range_usage' in self.trajectory_metrics_df.columns:
            sns.boxplot(data=self.trajectory_metrics_df, x='algorithm', y='joint_range_usage', ax=axes[2, 1])
            axes[2, 1].set_title('Joint Range Usage')
            axes[2, 1].set_ylabel('Normalized Joint Range Usage')
        
        # 9. Velocity smoothness
        if 'velocity_smoothness' in self.trajectory_metrics_df.columns:
            sns.boxplot(data=self.trajectory_metrics_df, x='algorithm', y='velocity_smoothness', ax=axes[2, 2])
            axes[2, 2].set_title('Velocity Smoothness (Lower = Smoother)')
            axes[2, 2].set_ylabel('Sum of Squared Vel. Changes')
            axes[2, 2].set_yscale('log')
        
        plt.tight_layout()
        plt.savefig(f'{self.plots_dir}/trajectory_quality_analysis.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        # Print smoothness summary
        print("\nüìä Smoothness Summary:")
        for algorithm in self.trajectory_metrics_df['algorithm'].unique():
            alg_data = self.trajectory_metrics_df[self.trajectory_metrics_df['algorithm'] == algorithm]
            print(f"  {algorithm.upper()}:")
            
            if 'smoothness_score' in alg_data.columns:
                smoothness = alg_data['smoothness_score']
                print(f"    Smoothness Score: {smoothness.mean():.4f} ¬± {smoothness.std():.4f}")
            
            if 'avg_jerk' in alg_data.columns:
                jerk = alg_data['avg_jerk']
                print(f"    Average Jerk: {jerk.mean():.4f} ¬± {jerk.std():.4f} rad/s¬≥")
            
            if 'jerk_smoothness' in alg_data.columns:
                jerk_smooth = alg_data['jerk_smoothness']
                print(f"    Jerk Smoothness: {jerk_smooth.mean():.4f} ¬± {jerk_smooth.std():.4f}")
            
            print()
    
    def analyze_joint_variance(self):
        """Enhanced analysis of joint angle variance across multiple trials."""
        print("\nüéØ Analyzing joint angle variance across multiple trials...")
        
        # Group by algorithm and pose to analyze variance
        variance_data = []
        joint_specific_variance = []
        
        for algorithm in self.results_df['algorithm'].unique():
            for pose_name in self.results_df['pose_name'].unique():
                # Get all trials for this algorithm-pose combination
                trials = self.results_df[
                    (self.results_df['algorithm'] == algorithm) & 
                    (self.results_df['pose_name'] == pose_name) &
                    (self.results_df['success'] == 1)
                ]
                
                if len(trials) < 2:
                    continue
                
                # Calculate variance in end configurations
                end_configs = []
                start_configs = []
                
                for _, trial in trials.iterrows():
                    end_config = [trial[f'end_config_{i}'] for i in range(7)]
                    start_config = [trial[f'start_config_{i}'] for i in range(7)]
                    end_configs.append(end_config)
                    start_configs.append(start_config)
                
                if len(end_configs) >= 2:
                    end_configs = np.array(end_configs)
                    start_configs = np.array(start_configs)
                    
                    # Calculate joint-specific variances
                    joint_variances = np.var(end_configs, axis=0)
                    start_variances = np.var(start_configs, axis=0)
                    
                    # Store overall variance data
                    variance_data.append({
                        'algorithm': algorithm,
                        'pose_name': pose_name,
                        'num_trials': len(trials),
                        'mean_end_variance': np.mean(joint_variances),
                        'max_end_variance': np.max(joint_variances),
                        'total_end_variance': np.sum(joint_variances),
                        'mean_start_variance': np.mean(start_variances),
                        'consistency_score': 1.0 / (1.0 + np.mean(joint_variances))  # Higher = more consistent
                    })
                    
                    # Store joint-specific variance data
                    for joint_idx in range(7):
                        joint_specific_variance.append({
                            'algorithm': algorithm,
                            'pose_name': pose_name,
                            'joint': f'J{joint_idx}',
                            'joint_index': joint_idx,
                            'end_variance': joint_variances[joint_idx],
                            'start_variance': start_variances[joint_idx],
                            'num_trials': len(trials)
                        })
        
        if variance_data:
            variance_df = pd.DataFrame(variance_data)
            joint_variance_df = pd.DataFrame(joint_specific_variance)
            
            # Create comprehensive variance analysis plots
            fig, axes = plt.subplots(2, 3, figsize=(20, 12))
            fig.suptitle('Joint Angle Variance Analysis Across Multiple Trials', fontsize=16, fontweight='bold')
            
            # 1. Mean end configuration variance
            sns.boxplot(data=variance_df, x='algorithm', y='mean_end_variance', ax=axes[0, 0])
            axes[0, 0].set_title('Mean Joint Angle Variance (End Config)')
            axes[0, 0].set_ylabel('Variance (rad¬≤)')
            
            # 2. Maximum end configuration variance
            sns.boxplot(data=variance_df, x='algorithm', y='max_end_variance', ax=axes[0, 1])
            axes[0, 1].set_title('Maximum Joint Angle Variance (End Config)')
            axes[0, 1].set_ylabel('Variance (rad¬≤)')
            
            # 3. Consistency score
            sns.boxplot(data=variance_df, x='algorithm', y='consistency_score', ax=axes[0, 2])
            axes[0, 2].set_title('Consistency Score (Higher = More Consistent)')
            axes[0, 2].set_ylabel('Consistency Score')
            
            # 4. Joint-specific variance comparison
            sns.boxplot(data=joint_variance_df, x='joint', y='end_variance', hue='algorithm', ax=axes[1, 0])
            axes[1, 0].set_title('Variance by Joint')
            axes[1, 0].set_ylabel('End Config Variance (rad¬≤)')
            axes[1, 0].legend(bbox_to_anchor=(1.05, 1), loc='upper left')
            
            # 5. Variance by pose
            sns.boxplot(data=variance_df, x='pose_name', y='mean_end_variance', hue='algorithm', ax=axes[1, 1])
            axes[1, 1].set_title('Variance by Pose')
            axes[1, 1].set_ylabel('Mean End Config Variance (rad¬≤)')
            axes[1, 1].tick_params(axis='x', rotation=45)
            axes[1, 1].legend(bbox_to_anchor=(1.05, 1), loc='upper left')
            
            # 6. Total variance comparison
            sns.barplot(data=variance_df, x='algorithm', y='total_end_variance', ax=axes[1, 2])
            axes[1, 2].set_title('Total Joint Variance')
            axes[1, 2].set_ylabel('Total Variance (rad¬≤)')
            
            plt.tight_layout()
            plt.savefig(f'{self.plots_dir}/joint_variance_analysis.png', dpi=300, bbox_inches='tight')
            plt.close()
            
            # Print variance summary
            print("\nüìä Joint Variance Summary:")
            for algorithm in variance_df['algorithm'].unique():
                alg_data = variance_df[variance_df['algorithm'] == algorithm]
                print(f"  {algorithm.upper()}:")
                print(f"    Mean End Config Variance: {alg_data['mean_end_variance'].mean():.6f} ¬± {alg_data['mean_end_variance'].std():.6f} rad¬≤")
                print(f"    Consistency Score: {alg_data['consistency_score'].mean():.4f} ¬± {alg_data['consistency_score'].std():.4f}")
                print(f"    Number of Pose-Algorithm Combinations: {len(alg_data)}")
                print()
            
            print(f"‚úì Analyzed variance for {len(variance_df)} algorithm-pose combinations")
            print(f"‚úì Analyzed {len(joint_variance_df)} joint-specific variance measurements")
        else:
            print("‚ö†Ô∏è  Insufficient data for variance analysis (need multiple trials per pose)")
    
    def analyze_joint_variance_over_trajectory(self):
        """Analyze and plot joint angle variance over trajectory time for each joint."""
        print("\nüìä Analyzing joint angle variance over trajectory time...")
        
        if not self.trajectory_data:
            print("‚ö†Ô∏è  No trajectory data available for variance analysis")
            return
        
        # Group trajectories by algorithm and pose
        trajectory_groups = {}
        for filename, traj_df in self.trajectory_data.items():
            try:
                parts = filename.replace('.csv', '').split('_')
                if len(parts) < 3:
                    continue
                    
                algorithm = parts[0]
                pose_name = parts[1]
                
                key = f"{algorithm}_{pose_name}"
                if key not in trajectory_groups:
                    trajectory_groups[key] = []
                trajectory_groups[key].append(traj_df)
            except Exception as e:
                print(f"‚ö†Ô∏è  Error processing {filename}: {e}")
                continue
        
        if not trajectory_groups:
            print("‚ö†Ô∏è  No valid trajectory groups found for variance analysis")
            return
        
        # Find trajectory groups with multiple trials for the same pose
        variance_groups = {}
        for key, trajectories in trajectory_groups.items():
            if len(trajectories) >= 1:  # Even single trajectories can show variance over time
                variance_groups[key] = trajectories
        
        if not variance_groups:
            print("‚ö†Ô∏è  No trajectory groups found for variance analysis")
            return
        
        # Create joint variance over time plots
        joint_cols = ['j0', 'j1', 'j2', 'j3', 'j4', 'j5', 'j6']
        joint_names = ['Shoulder Pan', 'Shoulder Lift', 'Elbow', 'Wrist 1', 'Wrist 2', 'Wrist 3', 'Wrist 4']
        
        # Create plots for each pose that has both algorithms
        poses_with_both_algorithms = set()
        algorithm_pose_data = {}
        
        for key in variance_groups.keys():
            algorithm, pose = key.split('_', 1)
            if pose not in algorithm_pose_data:
                algorithm_pose_data[pose] = {}
            algorithm_pose_data[pose][algorithm] = variance_groups[key]
            
            # Check if this pose has both algorithms
            if len(algorithm_pose_data[pose]) == 2:
                poses_with_both_algorithms.add(pose)
        
        if not poses_with_both_algorithms:
            print("‚ö†Ô∏è  No poses found with both STOMP and Hauser trajectories for comparison")
            return
        
        # Create plots for each pose
        for pose in poses_with_both_algorithms:
            self._plot_joint_variance_for_pose(pose, algorithm_pose_data[pose], joint_cols, joint_names)
        
        print(f"‚úì Generated joint variance over trajectory plots for {len(poses_with_both_algorithms)} poses")
    
    def _plot_joint_variance_for_pose(self, pose_name, algorithm_data, joint_cols, joint_names):
        """Plot joint angle variance over trajectory time for a specific pose."""
        fig, axes = plt.subplots(4, 2, figsize=(20, 24))
        fig.suptitle(f'Joint Angle Variance Over Trajectory - {pose_name}', fontsize=16, fontweight='bold')
        
        axes = axes.flatten()
        
        # ComparisonIK color scheme
        colors = {'STOMP': '#1f77b4', 'Hauser': '#ff7f0e'}  # Blue, Orange
        
        for joint_idx, (joint_col, joint_name) in enumerate(zip(joint_cols, joint_names)):
            ax = axes[joint_idx]
            
            for algorithm, trajectories in algorithm_data.items():
                if not trajectories:
                    continue
                
                # Get joint angle data for all trajectories of this algorithm
                joint_data_over_time = []
                max_length = 0
                
                for traj_df in trajectories:
                    if joint_col in traj_df.columns:
                        joint_angles = traj_df[joint_col].values
                        joint_data_over_time.append(joint_angles)
                        max_length = max(max_length, len(joint_angles))
                
                if not joint_data_over_time:
                    continue
                
                # Interpolate all trajectories to the same length for comparison
                normalized_trajectories = []
                for joint_angles in joint_data_over_time:
                    if len(joint_angles) > 1:
                        # Interpolate to max_length
                        old_indices = np.linspace(0, 1, len(joint_angles))
                        new_indices = np.linspace(0, 1, max_length)
                        interpolated = np.interp(new_indices, old_indices, joint_angles)
                        normalized_trajectories.append(interpolated)
                
                if len(normalized_trajectories) == 0:
                    continue
                
                # Convert to numpy array for easier manipulation
                traj_array = np.array(normalized_trajectories)
                
                if len(normalized_trajectories) > 1:
                    # Calculate mean and std across multiple trajectories
                    mean_trajectory = np.mean(traj_array, axis=0)
                    std_trajectory = np.std(traj_array, axis=0)
                else:
                    # For single trajectory, calculate rolling variance
                    mean_trajectory = traj_array[0]
                    # Calculate rolling standard deviation over a window
                    window_size = max(5, len(mean_trajectory) // 20)
                    std_trajectory = np.array([
                        np.std(mean_trajectory[max(0, i-window_size//2):i+window_size//2+1]) 
                        for i in range(len(mean_trajectory))
                    ])
                
                # Time vector (normalized from 0 to 1)
                time_normalized = np.linspace(0, 1, len(mean_trajectory))
                
                # Color for this algorithm
                color = colors.get(algorithm, '#1f77b4')
                alpha = 0.3
                
                # Plot mean trajectory with thicker line
                ax.plot(time_normalized, mean_trajectory, color=color, linewidth=3, 
                       label=f'{algorithm} Mean', alpha=0.9)
                
                # Plot variance as shaded area
                ax.fill_between(time_normalized, 
                               mean_trajectory - std_trajectory,
                               mean_trajectory + std_trajectory,
                               color=color, alpha=alpha, 
                               label=f'{algorithm} ¬±1œÉ')
                
                # Plot individual trajectories as thin lines
                for i, traj in enumerate(normalized_trajectories):
                    ax.plot(time_normalized, traj, color=color, linewidth=1, alpha=0.5)
            
            ax.set_title(f'{joint_name} (Joint {joint_idx})', fontsize=14, fontweight='bold')
            ax.set_xlabel('Normalized Trajectory Time')
            ax.set_ylabel('Joint Angle (rad)')
            ax.legend(frameon=True, fancybox=True, shadow=True)
            ax.grid(True, alpha=0.3)
            
            # Set median lines to black (ComparisonIK style)
            for line in ax.lines:
                if hasattr(line, 'get_linestyle') and line.get_linestyle() == '-':
                    if 'Mean' in line.get_label():
                        line.set_color('black')
        
        # Hide the last subplot if we have 7 joints (odd number)
        if len(joint_cols) == 7:
            axes[7].set_visible(False)
        
        plt.tight_layout()
        plt.savefig(f'{self.plots_dir}/joint_variance_over_trajectory_{pose_name}.png', 
                   dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"‚úì Saved joint variance plot for {pose_name}")
    
    def generate_summary_report(self):
        """Generate a comprehensive summary report."""
        print("\nüìã Generating summary report...")
        
        report_lines = []
        report_lines.append("ENHANCED STOMP vs HAUSER COMPARISON REPORT")
        report_lines.append("=" * 50)
        report_lines.append("")
        
        # Basic statistics
        total_trials = len(self.results_df)
        successful_trials = len(self.results_df[self.results_df['success'] == 1])
        report_lines.append(f"Total Trials: {total_trials}")
        report_lines.append(f"Successful Trials: {successful_trials} ({100*successful_trials/total_trials:.1f}%)")
        report_lines.append("")
        
        # Algorithm comparison
        for algorithm in self.results_df['algorithm'].unique():
            alg_data = self.results_df[self.results_df['algorithm'] == algorithm]
            alg_success = alg_data[alg_data['success'] == 1]
            
            report_lines.append(f"{algorithm.upper()} ALGORITHM:")
            report_lines.append(f"  Trials: {len(alg_data)}")
            report_lines.append(f"  Success Rate: {len(alg_success)}/{len(alg_data)} ({100*len(alg_success)/len(alg_data):.1f}%)")
            
            if len(alg_success) > 0:
                times = alg_success['planning_time_ms']
                report_lines.append(f"  Planning Time: {times.mean():.1f}¬±{times.std():.1f} ms")
                report_lines.append(f"  Min/Max Time: {times.min():.1f}/{times.max():.1f} ms")
            
            report_lines.append("")
        
        # Trajectory quality summary
        if not self.trajectory_metrics_df.empty:
            report_lines.append("TRAJECTORY QUALITY SUMMARY:")
            
            for algorithm in self.trajectory_metrics_df['algorithm'].unique():
                alg_metrics = self.trajectory_metrics_df[self.trajectory_metrics_df['algorithm'] == algorithm]
                
                report_lines.append(f"  {algorithm.upper()}:")
                if 'smoothness_score' in alg_metrics.columns:
                    smoothness = alg_metrics['smoothness_score']
                    report_lines.append(f"    Smoothness: {smoothness.mean():.3f}¬±{smoothness.std():.3f}")
                
                if 'path_length' in alg_metrics.columns:
                    path_len = alg_metrics['path_length']
                    report_lines.append(f"    Path Length: {path_len.mean():.3f}¬±{path_len.std():.3f} rad")
            
            report_lines.append("")
        
        # Write report to file
        report_file = f"{self.plots_dir}/analysis_report.txt"
        with open(report_file, 'w') as f:
            f.write('\n'.join(report_lines))
        
        print(f"‚úì Summary report saved to: {report_file}")
        
        # Print key findings to console
        print("\nüèÜ KEY FINDINGS:")
        for line in report_lines:
            if line.strip() and not line.startswith("="):
                print(f"  {line}")
    
    def run_full_analysis(self):
        """Run the complete analysis pipeline."""
        print("üöÄ Starting enhanced trajectory analysis...")
        
        try:
            self.load_data()
            self.compute_trajectory_metrics()
            self.calculate_clearance_metrics()  # Add clearance analysis
            self.analyze_computation_times()
            self.analyze_trajectory_execution_times()
            self.analyze_trajectory_quality()
            self.analyze_joint_variance()
            self.analyze_joint_variance_over_trajectory()
            self.generate_summary_report()
            
            print(f"\n‚úÖ Analysis completed! Check the '{self.plots_dir}/' directory for results.")
            
        except Exception as e:
            print(f"\n‚ùå Analysis failed: {e}")
            raise

def main():
    if len(sys.argv) != 3:
        print("Usage: python3 enhanced_analysis.py <results_csv> <trajectories_dir>")
        print("Example: python3 enhanced_analysis.py results/comparison_results_20231216_143022.csv results/trajectories_20231216_143022")
        sys.exit(1)
    
    results_file = sys.argv[1]
    trajectories_dir = sys.argv[2]
    
    analyzer = TrajectoryAnalyzer(results_file, trajectories_dir)
    analyzer.run_full_analysis()

if __name__ == "__main__":
    main()
