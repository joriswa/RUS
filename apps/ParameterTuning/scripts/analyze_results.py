#!/usr/bin/env python3
"""
Parameter Tuning Results Analysis and Visualization

This script analyzes the results from the parameter tuning optimization,
creates visualizations, and provides detailed insights into the performance
of STOMP vs Hauser trajectory planners.
"""

import json
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
from pathlib import Path
import logging
from typing import Dict, List, Tuple

def setup_logging():
    """Setup logging configuration."""
    logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')
    return logging.getLogger(__name__)

def load_results(results_dir: str = "simplified_tuning_results") -> Tuple[Dict, Dict]:
    """Load optimization results from JSON files."""
    results_path = Path(results_dir)
    
    stomp_file = results_path / "stomp_results.json"
    hauser_file = results_path / "hauser_results.json"
    
    stomp_results = None
    hauser_results = None
    
    if stomp_file.exists():
        with open(stomp_file, 'r') as f:
            stomp_results = json.load(f)
    
    if hauser_file.exists():
        with open(hauser_file, 'r') as f:
            hauser_results = json.load(f)
    
    return stomp_results, hauser_results

def analyze_convergence(results: Dict, algorithm: str) -> pd.DataFrame:
    """Analyze optimization convergence."""
    if not results or 'trials' not in results:
        return pd.DataFrame()
    
    trials_data = []
    best_so_far = float('inf')
    
    for trial in results['trials']:
        if trial['value'] is not None:
            if trial['value'] < best_so_far:
                best_so_far = trial['value']
            
            trials_data.append({
                'trial': trial['number'],
                'objective': trial['value'],
                'best_so_far': best_so_far,
                'algorithm': algorithm
            })
    
    return pd.DataFrame(trials_data)

def analyze_parameter_importance(results: Dict) -> pd.DataFrame:
    """Analyze parameter importance based on trial results."""
    if not results or 'trials' not in results:
        return pd.DataFrame()
    
    trials_data = []
    for trial in results['trials']:
        if trial['value'] is not None and trial['params']:
            row = {'objective': trial['value']}
            row.update(trial['params'])
            trials_data.append(row)
    
    return pd.DataFrame(trials_data)

def create_convergence_plot(stomp_conv: pd.DataFrame, hauser_conv: pd.DataFrame, 
                          output_dir: Path):
    """Create convergence comparison plot."""
    plt.figure(figsize=(12, 8))
    
    if not stomp_conv.empty:
        plt.subplot(2, 2, 1)
        plt.plot(stomp_conv['trial'], stomp_conv['objective'], 'o-', alpha=0.6, label='Trial Values')
        plt.plot(stomp_conv['trial'], stomp_conv['best_so_far'], 'r-', linewidth=2, label='Best So Far')
        plt.title('STOMP Optimization Convergence')
        plt.xlabel('Trial Number')
        plt.ylabel('Objective Value')
        plt.legend()
        plt.grid(True, alpha=0.3)
    
    if not hauser_conv.empty:
        plt.subplot(2, 2, 2)
        plt.plot(hauser_conv['trial'], hauser_conv['objective'], 'o-', alpha=0.6, label='Trial Values')
        plt.plot(hauser_conv['trial'], hauser_conv['best_so_far'], 'r-', linewidth=2, label='Best So Far')
        plt.title('Hauser Optimization Convergence')
        plt.xlabel('Trial Number')
        plt.ylabel('Objective Value')
        plt.legend()
        plt.grid(True, alpha=0.3)
    
    # Combined comparison
    plt.subplot(2, 1, 2)
    if not stomp_conv.empty:
        plt.plot(stomp_conv['trial'], stomp_conv['best_so_far'], 'b-', linewidth=2, label='STOMP Best')
    if not hauser_conv.empty:
        plt.plot(hauser_conv['trial'], hauser_conv['best_so_far'], 'r-', linewidth=2, label='Hauser Best')
    
    plt.title('Algorithm Comparison: Best Objective Over Time')
    plt.xlabel('Trial Number')
    plt.ylabel('Best Objective Value')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(output_dir / 'convergence_analysis.png', dpi=300, bbox_inches='tight')
    plt.close()

def create_parameter_distribution_plots(stomp_params: pd.DataFrame, hauser_params: pd.DataFrame, 
                                       output_dir: Path):
    """Create parameter distribution plots."""
    
    # STOMP parameter distributions
    if not stomp_params.empty:
        numeric_params = stomp_params.select_dtypes(include=[np.number]).drop('objective', axis=1)
        n_params = len(numeric_params.columns)
        
        if n_params > 0:
            fig, axes = plt.subplots(2, (n_params + 1) // 2, figsize=(15, 8))
            axes = axes.flatten() if n_params > 1 else [axes]
            
            for i, param in enumerate(numeric_params.columns):
                if i < len(axes):
                    # Color points by objective value
                    scatter = axes[i].scatter(stomp_params[param], stomp_params['objective'], 
                                            c=stomp_params['objective'], cmap='viridis', alpha=0.6)
                    axes[i].set_xlabel(param)
                    axes[i].set_ylabel('Objective Value')
                    axes[i].set_title(f'STOMP: {param} vs Objective')
                    plt.colorbar(scatter, ax=axes[i])
            
            # Hide unused subplots
            for i in range(n_params, len(axes)):
                axes[i].set_visible(False)
            
            plt.suptitle('STOMP Parameter Analysis')
            plt.tight_layout()
            plt.savefig(output_dir / 'stomp_parameter_analysis.png', dpi=300, bbox_inches='tight')
            plt.close()
    
    # Hauser parameter distributions
    if not hauser_params.empty:
        numeric_params = hauser_params.select_dtypes(include=[np.number]).drop('objective', axis=1)
        n_params = len(numeric_params.columns)
        
        if n_params > 0:
            fig, axes = plt.subplots(2, (n_params + 1) // 2, figsize=(15, 8))
            axes = axes.flatten() if n_params > 1 else [axes]
            
            for i, param in enumerate(numeric_params.columns):
                if i < len(axes):
                    scatter = axes[i].scatter(hauser_params[param], hauser_params['objective'], 
                                            c=hauser_params['objective'], cmap='viridis', alpha=0.6)
                    axes[i].set_xlabel(param)
                    axes[i].set_ylabel('Objective Value')
                    axes[i].set_title(f'Hauser: {param} vs Objective')
                    plt.colorbar(scatter, ax=axes[i])
            
            # Hide unused subplots
            for i in range(n_params, len(axes)):
                axes[i].set_visible(False)
            
            plt.suptitle('Hauser Parameter Analysis')
            plt.tight_layout()
            plt.savefig(output_dir / 'hauser_parameter_analysis.png', dpi=300, bbox_inches='tight')
            plt.close()

def create_comparison_summary(stomp_results: Dict, hauser_results: Dict, output_dir: Path):
    """Create algorithm comparison summary."""
    
    # Comparison data
    comparison_data = []
    
    if stomp_results:
        comparison_data.append({
            'Algorithm': 'STOMP',
            'Best Objective': stomp_results.get('best_objective', 'N/A'),
            'Trials': len(stomp_results.get('trials', [])),
            'Best Parameters': str(stomp_results.get('best_parameters', {}))
        })
    
    if hauser_results:
        comparison_data.append({
            'Algorithm': 'Hauser',
            'Best Objective': hauser_results.get('best_objective', 'N/A'),
            'Trials': len(hauser_results.get('trials', [])),
            'Best Parameters': str(hauser_results.get('best_parameters', {}))
        })
    
    # Create comparison plot
    if len(comparison_data) == 2:
        algorithms = [d['Algorithm'] for d in comparison_data]
        objectives = [d['Best Objective'] for d in comparison_data if isinstance(d['Best Objective'], (int, float))]
        
        if len(objectives) == 2:
            plt.figure(figsize=(10, 6))
            
            colors = ['#2E86C1', '#E74C3C']
            bars = plt.bar(algorithms, objectives, color=colors, alpha=0.7, edgecolor='black')
            
            # Add value labels on bars
            for bar, obj in zip(bars, objectives):
                plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.00001, 
                        f'{obj:.6f}', ha='center', va='bottom', fontweight='bold')
            
            plt.title('Algorithm Performance Comparison', fontsize=16, fontweight='bold')
            plt.ylabel('Best Objective Value (Lower is Better)')
            plt.grid(True, alpha=0.3, axis='y')
            
            # Determine winner
            winner_idx = np.argmin(objectives)
            winner = algorithms[winner_idx]
            plt.text(0.5, max(objectives) * 0.8, f'ðŸ† Winner: {winner}', 
                    transform=plt.gca().transData, ha='center', fontsize=14, 
                    bbox=dict(boxstyle="round,pad=0.3", facecolor="gold", alpha=0.8))
            
            plt.tight_layout()
            plt.savefig(output_dir / 'algorithm_comparison.png', dpi=300, bbox_inches='tight')
            plt.close()
    
    return comparison_data

def generate_markdown_report(stomp_results: Dict, hauser_results: Dict, 
                           comparison_data: List[Dict], output_dir: Path):
    """Generate a comprehensive markdown report."""
    
    report_content = """# Parameter Tuning Results Analysis

## Executive Summary

This report presents the results of parameter optimization for trajectory planning algorithms in ultrasound scanning applications. The optimization was performed using real scenario data with actual motion generation libraries.

"""
    
    # Algorithm Comparison Section
    report_content += "## Algorithm Performance Comparison\n\n"
    
    if len(comparison_data) >= 2:
        stomp_obj = comparison_data[0]['Best Objective']
        hauser_obj = comparison_data[1]['Best Objective']
        
        if isinstance(stomp_obj, (int, float)) and isinstance(hauser_obj, (int, float)):
            if stomp_obj < hauser_obj:
                winner = "STOMP"
                improvement = ((hauser_obj - stomp_obj) / hauser_obj) * 100
            else:
                winner = "Hauser"
                improvement = ((stomp_obj - hauser_obj) / stomp_obj) * 100
            
            report_content += f"### ðŸ† Winner: {winner}\n\n"
            report_content += f"- **Performance Improvement**: {improvement:.4f}%\n"
            report_content += f"- **STOMP Best Objective**: {stomp_obj:.6f}\n"
            report_content += f"- **Hauser Best Objective**: {hauser_obj:.6f}\n\n"
    
    # Detailed Results
    report_content += "## Detailed Results\n\n"
    
    if stomp_results:
        report_content += "### STOMP Optimization Results\n\n"
        report_content += f"- **Best Objective**: {stomp_results.get('best_objective', 'N/A'):.6f}\n"
        report_content += f"- **Total Trials**: {len(stomp_results.get('trials', []))}\n"
        report_content += "- **Optimal Parameters**:\n"
        
        best_params = stomp_results.get('best_parameters', {})
        for param, value in best_params.items():
            if isinstance(value, float):
                report_content += f"  - `{param}`: {value:.6f}\n"
            else:
                report_content += f"  - `{param}`: {value}\n"
        report_content += "\n"
    
    if hauser_results:
        report_content += "### Hauser Optimization Results\n\n"
        report_content += f"- **Best Objective**: {hauser_results.get('best_objective', 'N/A'):.6f}\n"
        report_content += f"- **Total Trials**: {len(hauser_results.get('trials', []))}\n"
        report_content += "- **Optimal Parameters**:\n"
        
        best_params = hauser_results.get('best_parameters', {})
        for param, value in best_params.items():
            if isinstance(value, float):
                report_content += f"  - `{param}`: {value:.6f}\n"
            else:
                report_content += f"  - `{param}`: {value}\n"
        report_content += "\n"
    
    # Technical Analysis
    report_content += """## Technical Analysis

### Objective Function

The optimization minimized a composite objective function that balances:

- **Planning Time** (30% weight): Lower planning times are preferred
- **Success Rate** (25% weight): Higher success rates are preferred  
- **Path Length** (20% weight): Shorter, more efficient paths are preferred
- **Safety Score** (15% weight): Higher safety margins are preferred
- **Smoothness Score** (10% weight): Smoother trajectories are preferred

### Test Scenarios

The optimization was performed using real ultrasound scanning scenarios:

1. **Simple Scenario**: 3 poses from scenario_1 data
2. **Medium Scenario**: 8 poses from scenario_1 data

Each scenario includes:
- Real robot arm configurations
- Actual obstacle environments
- Contact-based ultrasound probe positioning
- Physical constraints and safety margins

### Methodology

- **Optimizer**: Optuna with TPE (Tree-structured Parzen Estimator)
- **Trials per Algorithm**: 50
- **Evaluation**: Actual C++ motion generation libraries
- **Metrics**: Real trajectory planning performance

## Recommendations

Based on the optimization results:

"""
    
    if len(comparison_data) >= 2:
        stomp_obj = comparison_data[0]['Best Objective']
        hauser_obj = comparison_data[1]['Best Objective']
        
        if isinstance(stomp_obj, (int, float)) and isinstance(hauser_obj, (int, float)):
            if stomp_obj < hauser_obj:
                report_content += """1. **Use STOMP** for ultrasound scanning applications
2. Apply the optimized parameters identified in this study
3. Consider STOMP's stochastic approach benefits for complex environments
4. Monitor performance in production scenarios

"""
            else:
                report_content += """1. **Use Hauser** for ultrasound scanning applications
2. Apply the optimized parameters identified in this study
3. Leverage Hauser's deterministic parabolic ramp approach
4. Monitor performance in production scenarios

"""
    
    report_content += """## Generated Visualizations

This analysis generated the following plots:

- `algorithm_comparison.png`: Direct performance comparison
- `convergence_analysis.png`: Optimization convergence over trials
- `stomp_parameter_analysis.png`: STOMP parameter sensitivity analysis
- `hauser_parameter_analysis.png`: Hauser parameter sensitivity analysis

## Conclusion

The parameter optimization successfully identified optimal configurations for both trajectory planning algorithms using real motion generation libraries and actual scenario data. The results provide concrete parameter recommendations for ultrasound scanning applications.

---

*Report generated automatically from parameter tuning optimization results.*
"""
    
    # Save report
    with open(output_dir / 'optimization_report.md', 'w') as f:
        f.write(report_content)

def main():
    """Main analysis function."""
    logger = setup_logging()
    logger.info("Starting parameter tuning results analysis...")
    
    # Load results
    stomp_results, hauser_results = load_results()
    
    if not stomp_results and not hauser_results:
        logger.error("No results found to analyze!")
        return 1
    
    # Create output directory for analysis
    output_dir = Path("tuning_analysis")
    output_dir.mkdir(exist_ok=True)
    
    # Analyze convergence
    stomp_conv = analyze_convergence(stomp_results, 'STOMP') if stomp_results else pd.DataFrame()
    hauser_conv = analyze_convergence(hauser_results, 'Hauser') if hauser_results else pd.DataFrame()
    
    # Analyze parameter importance
    stomp_params = analyze_parameter_importance(stomp_results) if stomp_results else pd.DataFrame()
    hauser_params = analyze_parameter_importance(hauser_results) if hauser_results else pd.DataFrame()
    
    # Create visualizations
    logger.info("Creating convergence plots...")
    create_convergence_plot(stomp_conv, hauser_conv, output_dir)
    
    logger.info("Creating parameter analysis plots...")
    create_parameter_distribution_plots(stomp_params, hauser_params, output_dir)
    
    logger.info("Creating comparison summary...")
    comparison_data = create_comparison_summary(stomp_results, hauser_results, output_dir)
    
    # Generate comprehensive report
    logger.info("Generating markdown report...")
    generate_markdown_report(stomp_results, hauser_results, comparison_data, output_dir)
    
    # Print summary
    logger.info("\n" + "="*60)
    logger.info("PARAMETER TUNING ANALYSIS COMPLETE")
    logger.info("="*60)
    
    for data in comparison_data:
        logger.info(f"{data['Algorithm']:>10}: {data['Best Objective']:.6f} (best objective)")
    
    if len(comparison_data) >= 2:
        stomp_obj = comparison_data[0]['Best Objective']
        hauser_obj = comparison_data[1]['Best Objective']
        
        if isinstance(stomp_obj, (int, float)) and isinstance(hauser_obj, (int, float)):
            if stomp_obj < hauser_obj:
                winner = "STOMP"
                improvement = ((hauser_obj - stomp_obj) / hauser_obj) * 100
            else:
                winner = "Hauser"
                improvement = ((stomp_obj - hauser_obj) / stomp_obj) * 100
            
            logger.info(f"\nðŸ† WINNER: {winner}")
            logger.info(f"Performance improvement: {improvement:.4f}%")
    
    logger.info(f"\nAnalysis results saved to: {output_dir}/")
    logger.info("Generated files:")
    logger.info("  - optimization_report.md")
    logger.info("  - algorithm_comparison.png")
    logger.info("  - convergence_analysis.png")
    logger.info("  - stomp_parameter_analysis.png")
    logger.info("  - hauser_parameter_analysis.png")
    
    return 0

if __name__ == "__main__":
    exit(main())